{{- if gt (int .Values.replicas) 1 }}
################################
## Prometheus Service(s)
## each one pointing to a different
## Prometheus pod
#################################
{{ range $i, $e := until (int .Values.replicas) }}
{{- if gt $i 0 }}
---
{{- end }}
kind: Service
apiVersion: v1
metadata:
  name: {{ template "prometheus.fullname" $ }}-{{ $i }}
  labels:
    tier: monitoring
    component: {{ template "prometheus.name" $ }}
    chart: {{ template "prometheus.chart" $ }}
    release: {{ $.Release.Name }}
    heritage: {{ $.Release.Service }}
spec:
  type: ClusterIP
  selector:
    tier: monitoring
    component: {{ template "prometheus.name" $ }}
    release: {{ $.Release.Name }}
    # Prometheus is not designed to be horizontally scalable behind a load balancer.
    # Instead, replicas are assumed to used in an active/passive configuration
    # where a component querying prometheus may have application-layer logic
    # to retry for missing data on the next Service. With this configuration,
    # each Service will point to a different Pod.
    statefulset.kubernetes.io/pod-name: {{ template "prometheus.fullname" $ }}-{{ $i }}
  ports:
    - name: prometheus-data
      protocol: TCP
      port: {{ $.Values.ports.http }}
      targetPort: {{ $.Values.ports.http }}
{{ end }}
{{- end }}
---
################################
## Prometheus Service
## load balancing between all pods
#################################
kind: Service
apiVersion: v1
metadata:
  name: {{ template "prometheus.fullname" $ }}
  labels:
    tier: monitoring
    component: {{ template "prometheus.name" $ }}
    chart: {{ template "prometheus.chart" $ }}
    release: {{ $.Release.Name }}
    heritage: {{ $.Release.Service }}
spec:
  sessionAffinity: ClientIP
  type: ClusterIP
  selector:
    tier: monitoring
    component: {{ template "prometheus.name" $ }}
    release: {{ $.Release.Name }}
  ports:
    - name: prometheus-data
      protocol: TCP
      port: {{ $.Values.ports.http }}
      targetPort: {{ $.Values.ports.http }}
