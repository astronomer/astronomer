################################
## Prometheus Alerts ConfigMap
#################################

## Severity Definitions
# Info - goes to an info channel specific to the group that is interested. This should be very little
# Warning - goes to warning channel
#   things we want to know about but are not always actionable. Get a pulse of potential future issues
# High - Goes to slack prod alerts channel
#   Things that are actionable and should be taken care of but do not need to wake someone up
# Critical - goes to slack prod alerts channel and pager duty
# Things that would impact uptime or customer experience and are worth waking up in the middle of the night

kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ template "prometheus.fullname" . }}-alerts
  labels:
    tier: monitoring
    component: {{ template "prometheus.name" . }}
    chart: {{ template "prometheus.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  alerts: |-
    groups:
      - name: airflow
        rules:
        {{- if .Values.additionalAlerts.airflow }}
        {{- tpl .Values.additionalAlerts.airflow $ | nindent 8 }}
        {{- end }}
        - alert: AirflowDeploymentUnhealthy
          expr: sum by(release) (kube_pod_container_status_running{container=~".*(scheduler|scheduler-gc|webserver|worker|statsd|pgbouncer|metrics-exporter|redis|flower)"}) - count by(release) (kube_pod_container_status_running{container=~".*(scheduler|scheduler-gc|webserver|worker|statsd|pgbouncer|metrics-exporter|redis|flower)"}) < 0
          for: 15m # Rough number but should be enough to clear deployments with a reasonable amount of workers
          labels:
            tier: airflow
            component: deployment
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            # deployment label isn't populated by the query. However release and deployment are the same thing
            deployment: {{ printf "%q" "{{ $labels.release }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.release }} deployment is unhealthy" }}
            description: {{ printf "%q" "The {{ $labels.release }} deployment is not completely available." }}


        # This alert depends on the scheduler_heartbeat metric being a counter.
        # The type filter here was introduced in 0.7.0, so we don't trigger alerts for
        # older deployments.
        - alert: AirflowSchedulerUnhealthy
          expr: rate(airflow_scheduler_heartbeat{type="counter"}[1m]) == 0
          for: 6m
          labels:
            tier: airflow
            component: scheduler
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.deployment }} scheduler is unhealthy" }}
            description: {{ printf "%q" "The {{ $labels.deployment }} scheduler's heartbeat has dropped below the acceptable rate." }}

        - alert: AirflowPodQuota
          expr: (sum by (release) (kube_resourcequota{resource="pods", type="used"}) / sum by (release) (kube_resourcequota{resource="pods", type="hard"})*100) > 95
          for: 10m
          labels:
            tier: airflow
            component: deployment
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            # deployment label isn't populated by the query. However release and deployment are the same thing
            deployment: {{ printf "%q" "{{ $labels.release }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.release }} is near its pod quota" }}
            description: {{ printf "%q" "{{ $labels.release }} has been using over 95% of its pod quota for over 10 minutes." }}

        - alert: AirflowEphemeralStorageLimit
          expr: (container_fs_usage_bytes{pod_name=~".*(scheduler|webserver|worker).*"}) >= 8500000000
          for: 10m
          labels:
            tier: airflow
            component: {{ printf "%q" "{{ $labels.component_name }}" }}
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.deployment }} {{ $labels.component_name }} is experiencing high ephemeral storage usage" }}
            description: {{ printf "%q" "{{ $labels.deployment }} {{ $labels.component_name }} has been using >=8.5GB of its ephemeral storage for over 10 minutes." }}

        - alert: AirflowTasksPendingIncreasing
          expr: (max_over_time(airflow_scheduler_tasks_pending[5m]) - max_over_time(airflow_scheduler_tasks_pending[5m] offset 5m)) > 0
          for: 30m
          labels:
            tier: airflow
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.deployment }} is creating tasks faster than it's clearing them." }}
            description: {{ printf "%q" "{{ $labels.deployment }}: the number of pending tasks has been increasing for 30 minutes" }}

        - alert: ContainerMemoryNearTheLimitInDeployment
          expr: |
            (container_memory_working_set_bytes{container=~".*(scheduler|scheduler-gc|webserver|worker|statsd|pgbouncer|metrics-exporter|redis|flower)", container_name!~"POD|istio-proxy"} /
            container_spec_memory_limit_bytes{container=~".*(scheduler|scheduler-gc|webserver|worker|statsd|pgbouncer|metrics-exporter|redis|flower)", container_name!~"POD|istio-proxy"}) * 100 > 95 < +Inf
          for: 1h
          labels:
            tier: airflow
            component: container
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
            severity: warning
          annotations:
            summary: "A container is using more than 95% of the memory limit"
            description: {{ printf "%q" "container {{ $labels.container }} in pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using {{ printf \"%.2f\" $value }}% of available memory." }}

        - alert: CpuThrottlingInDeployment
          expr: 100 * (increase(container_cpu_cfs_throttled_periods_total{namespace!="{{ .Release.Name }}", container_name!~".*worker.*|()"}[5m]) / increase(container_cpu_cfs_periods_total{namespace!="{{ .Release.Name }}", namespace != "kube-system", container_name!~".*worker.*|()"}[5m])) > 75
          for: 5m
          labels:
            silence: cre
            #tier: airflow
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
            severity: warning
          annotations:
            summary: {{ printf "%q" "{{ $labels.pod_name }} ({{ $labels.container_name }}) in namespace {{ $labels.namespace }} is getting throttled {{ $value }}% of the time" }}
            description: "In the past 5 minutes, one or more components in the deployment are experiencing CPU throttling."

      - name: platform
        rules:
        {{- if .Values.additionalAlerts.platform }}
        {{- tpl .Values.additionalAlerts.platform $ | nindent 8 }}
        {{- end }}
        - alert: AirflowOperatorFailureRate
          expr: 100 * (sum by (operator) (increase(airflow_operator_failures{operator=~"([A-Z][a-z0-9]+)(([0-9])|([A-Z0-9][a-z0-9]+))*([A-Z])?Operator"}[1h])) / (sum by (operator) (increase(airflow_operator_successes{operator=~"([A-Z][a-z0-9]+)(([0-9])|([A-Z0-9][a-z0-9]+))*([A-Z])?Operator"}[1h])) + sum by (operator) (increase(airflow_operator_failures{operator=~"([A-Z][a-z0-9]+)(([0-9])|([A-Z0-9][a-z0-9]+))*([A-Z])?Operator"}[1h])))) > 50
          for: 2h
          labels:
            tier: platform
            severity: warn
            operator: {{ printf "%q" "{{ $labels.operator }}" }}
          annotations:
            summary: {{ printf "%q" "Across all deployments, {{ $labels.operator }} is failing {{ $value }}% of the time" }}
            description: "This alarm is used to notify support if an external data source may be inoperational, but the query is subject to be incorrect for small sample sizes where a single deployment uses an operator more than the rest of deployments combined over the course of two hours"

        - alert: ManyUnhealthySchedulers
          expr: count(rate(airflow_scheduler_heartbeat{}[1m]) <= 0) > 5
          for: 5m
          labels:
            tier: platform
            severity: critical
          annotations:
            summary: {{ printf "%q" "{{ $value }} airflow schedulers are not heartbeating" }}
            description: "If more than 5 Airflow Schedulers are not heartbeating for more than 5 minutes, this alarm fires."

        - alert: SchedulersNotHealthy
          expr: (count(rate(airflow_scheduler_heartbeat{}[1m]) > 0) / count(rate(airflow_scheduler_heartbeat{}[1m]))) < 0.5
          for: 5m
          labels:
            tier: platform
            component: airflow
            severity: critical
          annotations:
            summary: "Half or more of schedulers do not have a heartbeat"
            description: {{ printf "%q" "{{ $value }} }} schedulers do not have a heartbeat in the last five minutes" }}

      - name: node-exporter-recording.rules
        rules:
        - expr: |
            count without (cpu) (
              count without (mode) (
                node_cpu_seconds_total{job="node-exporter"}
              )
            )
          record: instance:node_num_cpu:sum
        - expr: |
            1 - avg without (cpu, mode) (
              rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
            )
          record: instance:node_cpu_utilisation:rate1m
        - expr: |
            (
              node_load1{job="node-exporter"}
            /
              instance:node_num_cpu:sum{job="node-exporter"}
            )
          record: instance:node_load1_per_cpu:ratio
        - expr: |
            1 - (
              node_memory_MemAvailable_bytes{job="node-exporter"}
            /
              node_memory_MemTotal_bytes{job="node-exporter"}
            )
          record: instance:node_memory_utilisation:ratio
        - expr: |
            rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
          record: instance:node_vmstat_pgmajfault:rate1m
        - expr: |
            rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
          record: instance_device:node_disk_io_time_seconds:rate1m
        - expr: |
            rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
          record: instance_device:node_disk_io_time_weighted_seconds:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_receive_bytes_excluding_lo:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_transmit_bytes_excluding_lo:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_receive_drop_excluding_lo:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_transmit_drop_excluding_lo:rate1m

      - name: k8s-recording.rules
        rules:
        - expr: |
            sum(rate(container_cpu_usage_seconds_total{job="kubernetes-nodes-cadvisor", image!="", container!="POD"}[5m])) by (namespace)
          record: namespace:container_cpu_usage_seconds_total:sum_rate
        - expr: |
            sum by (cluster, namespace, pod, container) (
              rate(container_cpu_usage_seconds_total{job="kubernetes-nodes-cadvisor", image!="", container!="POD"}[5m])
            ) * on (cluster, namespace, pod) group_left(node) max by(cluster, namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
        - expr: |
            container_memory_working_set_bytes{job="kubernetes-nodes-cadvisor", image!=""}
            * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_memory_working_set_bytes
        - expr: |
            container_memory_rss{job="kubernetes-nodes-cadvisor", image!=""}
            * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_memory_rss
        - expr: |
            container_memory_cache{job="kubernetes-nodes-cadvisor", image!=""}
            * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_memory_cache
        - expr: |
            container_memory_swap{job="kubernetes-nodes-cadvisor", image!=""}
            * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_memory_swap
        - expr: |
            sum(container_memory_usage_bytes{job="kubernetes-nodes-cadvisor", image!="", container!="POD"}) by (namespace)
          record: namespace:container_memory_usage_bytes:sum
        - expr: |
            sum by (namespace) (
                sum by (namespace, pod) (
                    max by (namespace, pod, container) (
                        kube_pod_container_resource_requests_memory_bytes{job="{{.Release.Name}}-kube-state"}
                    ) * on(namespace, pod) group_left() max by (namespace, pod) (
                        kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace:kube_pod_container_resource_requests_memory_bytes:sum
        - expr: |
            sum by (namespace) (
                sum by (namespace, pod) (
                    max by (namespace, pod, container) (
                        kube_pod_container_resource_requests_cpu_cores{job="{{.Release.Name}}-kube-state"}
                    ) * on(namespace, pod) group_left() max by (namespace, pod) (
                      kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace:kube_pod_container_resource_requests_cpu_cores:sum
        - expr: |
            sum(
              label_replace(
                label_replace(
                  kube_pod_owner{job="{{.Release.Name}}-kube-state", owner_kind="ReplicaSet"},
                  "replicaset", "$1", "owner_name", "(.*)"
                ) * on(replicaset, namespace) group_left(owner_name) kube_replicaset_owner{job="{{.Release.Name}}-kube-state"},
                "workload", "$1", "owner_name", "(.*)"
              )
            ) by (cluster, namespace, workload, pod)
          labels:
            workload_type: deployment
          record: mixin_pod_workload
        - expr: |
            sum(
              label_replace(
                kube_pod_owner{job="{{.Release.Name}}-kube-state", owner_kind="DaemonSet"},
                "workload", "$1", "owner_name", "(.*)"
              )
            ) by (cluster, namespace, workload, pod)
          labels:
            workload_type: daemonset
          record: mixin_pod_workload
        - expr: |
            sum(
              label_replace(
                kube_pod_owner{job="{{.Release.Name}}-kube-state", owner_kind="StatefulSet"},
                "workload", "$1", "owner_name", "(.*)"
              )
            ) by (cluster, namespace, workload, pod)
          labels:
            workload_type: statefulset
          record: mixin_pod_workload
      - name: kube-scheduler-recording.rules
        rules:
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - name: node-recording.rules
        rules:
        - expr: |
            sum(min(kube_pod_info) by (cluster, node))
          record: ':kube_pod_info_node_count:'
        - expr: |
            max(label_replace(kube_pod_info{job="{{.Release.Name}}-kube-state"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
          record: 'node_namespace_pod:kube_pod_info:'
        - expr: |
            count by (cluster, node) (sum by (node, cpu) (
              node_cpu_seconds_total{job="node-exporter"}
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            ))
          record: node:node_num_cpu:sum
        - expr: |
            sum(
              node_memory_MemAvailable_bytes{job="node-exporter"} or
              (
                node_memory_Buffers_bytes{job="node-exporter"} +
                node_memory_Cached_bytes{job="node-exporter"} +
                node_memory_MemFree_bytes{job="node-exporter"} +
                node_memory_Slab_bytes{job="node-exporter"}
              )
            ) by (cluster)
          record: :node_memory_MemAvailable_bytes:sum
      - name: kube-prometheus-node-recording.rules
        rules:
        - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m])) BY
            (instance)
          record: instance:node_cpu:rate:sum
        - expr: sum((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}))
            BY (instance)
          record: instance:node_filesystem_usage:sum
        - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
          record: instance:node_network_receive_bytes:rate:sum
        - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
          record: instance:node_network_transmit_bytes:rate:sum
        - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m])) WITHOUT
            (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total)
            BY (instance, cpu)) BY (instance)
          record: instance:node_cpu:ratio
        - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
          record: cluster:node_cpu:sum_rate5m
        - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total)
            BY (instance, cpu))
          record: cluster:node_cpu:ratio
      - name: kube-prometheus-general-recording.rules
        rules:
        - expr: count without(instance, pod, node) (up == 1)
          record: count:up1
        - expr: count without(instance, pod, node) (up == 0)
          record: count:up0
      - name: kube-state-metrics
        rules:
        - alert: KubeStateMetricsListErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in
              list operations. This is likely causing it to not be able to expose metrics
              about Kubernetes objects correctly or at all.
            summary: Errors with kube-state-metrics list operations
            runbook_url:
          expr: |
            (sum(rate(kube_state_metrics_list_total{job="{{.Release.Name}}-kube-state",result="error"}[5m]))
              /
            sum(rate(kube_state_metrics_list_total{job="{{.Release.Name}}-kube-state"}[5m])))
            > 0.01
          for: 15m
          labels:
            severity: critical
            tier: platform
        - alert: KubeStateMetricsWatchErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in
              watch operations. This is likely causing it to not be able to expose metrics
              about Kubernetes objects correctly or at all.
            runbook_url:
            summary: Errors with kube-state-metrics watch operations
          expr: |
            (sum(rate(kube_state_metrics_watch_total{job="{{.Release.Name}}-kube-state",result="error"}[5m]))
              /
            sum(rate(kube_state_metrics_watch_total{job="{{.Release.Name}}-kube-state"}[5m])))
            > 0.01
          for: 15m
          labels:
            severity: critical
            tier: platform

      - name: node-exporter
        rules:
        - alert: NodeFilesystemSpaceFillingUp
          annotations:
            description: Filesystem on {{`{{ $labels.device }}`}} at {{`{{ $labels.instance }}`}}
              has only {{`{{ printf "%.2f" $value }}`}}% available space left and is filling
              up fast.
            runbook_url:
            summary: Filesystem is predicted to run out of space within the next 4 hours.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 20
            and
              predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            severity: critical
            tier: platform
        - alert: NodeFilesystemAlmostOutOfSpace
          annotations:
            description: Filesystem on {{`{{ $labels.device }}`}} at {{`{{ $labels.instance }}`}}
              has only {{`{{ printf "%.2f" $value }}`}}% available space left.
            runbook_url:
            summary: Filesystem has less than 5% space left.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 5
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            severity: warning
            tier: platform
        - alert: NodeFilesystemAlmostOutOfSpace
          annotations:
            description: Filesystem on {{`{{ $labels.device }}`}} at {{`{{ $labels.instance }}`}}
              has only {{`{{ printf "%.2f" $value }}`}}% available space left.
            runbook_url:
            summary: Filesystem has less than 3% space left.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 3
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            severity: critical
            tier: platform
        - alert: NodeFilesystemFilesFillingUp
          annotations:
            description: Filesystem on {{`{{ $labels.device }}`}} at {{`{{ $labels.instance }}`}}
              has only {{`{{ printf "%.2f" $value }}`}}% available inodes left and is filling
              up.
            runbook_url:
            summary: Filesystem is predicted to run out of inodes within the next 24 hours.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 40
            and
              predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            severity: warning
            tier: platform
        - alert: NodeFilesystemFilesFillingUp
          annotations:
            description: Filesystem on {{`{{ $labels.device }}`}} at {{`{{ $labels.instance }}`}}
              has only {{`{{ printf "%.2f" $value }}`}}% available inodes left and is filling
              up fast.
            runbook_url:
            summary: Filesystem is predicted to run out of inodes within the next 4 hours.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 20
            and
              predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            severity: critical
        - alert: NodeFilesystemAlmostOutOfFiles
          annotations:
            description: Filesystem on {{`{{ $labels.device }}`}} at {{`{{ $labels.instance }}`}}
              has only {{`{{ printf "%.2f" $value }}`}}% available inodes left.
            runbook_url:
            summary: Filesystem has less than 5% inodes left.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 5
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            severity: warning
            tier: platform
        - alert: NodeFilesystemAlmostOutOfFiles
          annotations:
            description: Filesystem on {{`{{ $labels.device }}`}} at {{`{{ $labels.instance }}`}}
              has only {{`{{ printf "%.2f" $value }}`}}% available inodes left.
            runbook_url:
            summary: Filesystem has less than 3% inodes left.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            severity: critical
        - alert: NodeNetworkReceiveErrs
          annotations:
            description: '{{`{{ $labels.instance }}`}} interface {{`{{ $labels.device }}`}} has encountered
              {{`{{ printf "%.0f" $value }}`}} receive errors in the last two minutes.'
            runbook_url:
            summary: Network interface is reporting many receive errors.
          expr: |
            increase(node_network_receive_errs_total[2m]) > 10
          for: 1h
          labels:
            severity: warning
            tier: platform
        - alert: NodeNetworkTransmitErrs
          annotations:
            description: '{{`{{ $labels.instance }}`}} interface {{`{{ $labels.device }}`}} has encountered
              {{`{{ printf "%.0f" $value }}`}} transmit errors in the last two minutes.'
            runbook_url:
            summary: Network interface is reporting many transmit errors.
          expr: |
            increase(node_network_transmit_errs_total[2m]) > 10
          for: 1h
          labels:
            severity: warning
            tier: platform

      - name: kubernetes-apps
        rules:
        - alert: KubePodCrashLooping
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} ({{`{{ $labels.container
              }}`}}) is restarting {{`{{ printf "%.2f" $value }}`}} times / 5 minutes.
            runbook_url:
            summary: '{{`{{ $labels.pod }}`}} Pod is CrashLooping'
          expr: |
            rate(kube_pod_container_status_restarts_total{job="{{.Release.Name}}-kube-state"}[15m]) * 60 * 5 > 0
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubePodNotReady
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-ready
              state for longer than 15 minutes.
            runbook_url:
            summary: '{{`{{ $labels.pod }}`}} Pod in a non-ready state'
          expr: |
            sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="{{.Release.Name}}-kube-state", phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})) > 0
          for: 15m
          labels:
            severity: high
            tier: platform

        - alert: HighNumberOfPodsNotReady
          expr: count(kube_pod_status_phase{phase=~"Pending|Unknown"} > 0) > 25
          for: 20m
          labels:
            severity: critical
            tier: platform
          annotations:
            runbook_url:
            summary: "A high number of pods are not ready. A wide spread issue is likely"
            description: "A high number of pods are not ready. This is a signal that there might be a wide spread issue going on in the cluster or on a node."

        - alert: CriticalComponentPodCrashLooping
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} ({{`{{ $labels.container
              }}`}}) is restarting {{`{{ printf "%.2f" $value }}`}} times / 5 minutes.
            runbook_url:
            summary: '{{`{{ $labels.pod }}`}} Pod is CrashLooping'
          expr: |
            rate(kube_pod_container_status_restarts_total{container=~"astro-ui|commander|registry|houston|prisma|prometheus|sqlproxy"}[15m]) * 60 * 5 > 0
          for: 15m
          labels:
            severity: critical
            tier: platform

        - alert: CriticalComponentPodNotReady
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-ready
              state for longer than 15 minutes.
            runbook_url:
            summary: '{{`{{ $labels.pod }}`}} Pod in a non-ready state'
          expr: |
            sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{namespace="{{.Release.Namespace}}", phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})) > 0
          for: 15m
          labels:
            severity: critical
            tier: platform

        - alert: KubeDeploymentGenerationMismatch
          annotations:
            description: Deployment generation for {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment
              }}`}} does not match, this indicates that the Deployment has failed but has
              not been rolled back.
            runbook_url:
            summary: Deployment version mismatch
          expr: |
            kube_deployment_status_observed_generation{job="{{.Release.Name}}-kube-state"}
              !=
            kube_deployment_metadata_generation{job="{{.Release.Name}}-kube-state"}
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeDeploymentReplicasMismatch
          annotations:
            description: Deployment {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} has not
              matched the expected number of replicas for longer than 15 minutes.
            runbook_url:
            summary: '{{`{{ $labels.deployment }}`}} Replica count mismatch'
          expr: |
            kube_deployment_spec_replicas{job="{{.Release.Name}}-kube-state"}
              !=
            kube_deployment_status_replicas_available{job="{{.Release.Name}}-kube-state"}
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeStatefulSetReplicasMismatch
          annotations:
            description: StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} has
              not matched the expected number of replicas for longer than 15 minutes.
            runbook_url:
            summary: '{{`{{ $labels.statefulset }}`}} StatefulSet replica mismatch'
          expr: |
            kube_statefulset_status_replicas_ready{job="{{.Release.Name}}-kube-state"}
              !=
            kube_statefulset_status_replicas{job="{{.Release.Name}}-kube-state"}
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeStatefulSetGenerationMismatch
          annotations:
            description: StatefulSet generation for {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset
              }}`}} does not match, this indicates that the StatefulSet has failed but has
              not been rolled back.
            runbook_url:
            summary: '{{`{{ $labels.statefulset}}`}} Version mismatch'
          expr: |
            kube_statefulset_status_observed_generation{job="{{.Release.Name}}-kube-state"}
              !=
            kube_statefulset_metadata_generation{job="{{.Release.Name}}-kube-state"}
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeStatefulSetUpdateNotRolledOut
          annotations:
            description: StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} update
              has not been rolled out.
            runbook_url:
            summary: StatefulSet Rollout Error
          expr: |
            max without (revision) (
              kube_statefulset_status_current_revision{job="{{.Release.Name}}-kube-state"}
                unless
              kube_statefulset_status_update_revision{job="{{.Release.Name}}-kube-state"}
            )
              *
            (
              kube_statefulset_replicas{job="{{.Release.Name}}-kube-state"}
                !=
              kube_statefulset_status_replicas_updated{job="{{.Release.Name}}-kube-state"}
            )
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeDaemonSetRolloutStuck
          annotations:
            description: Only {{`{{ $value | humanizePercentage }}`}} of the desired Pods of DaemonSet
              {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset }}`}} are scheduled and ready.
            runbook_url:
            summary: DaemonSet not deployed on all expected nodes.
          expr: |
            kube_daemonset_status_number_ready{job="{{.Release.Name}}-kube-state"}
              /
            kube_daemonset_status_desired_number_scheduled{job="{{.Release.Name}}-kube-state"} < 1.00
            and
            increase(kube_daemonset_metadata_generation{job="{{.Release.Name}}-kube-state"}[10m]) == 0
          for: 15m
          labels:
            severity: critical
            tier: platform
        - alert: KubeContainerWaiting
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} container {{`{{ $labels.container}}`}}
              has been in waiting state for longer than 1 hour.
            runbook_url:
            summary: '{{`{{ $labels.container}}`}} Container is stuck in waiting state'
          expr: |
            sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="{{.Release.Name}}-kube-state"}) > 0
          for: 1h
          labels:
            severity: high
            tier: platform
        - alert: KubeDaemonSetNotScheduled
          annotations:
            description: '{{`{{ $value }}`}} Pods of DaemonSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset
              }}`}} are not scheduled.'
            runbook_url:
            summary: Not all pods of daemonset are scheduled
          expr: |
            kube_daemonset_status_desired_number_scheduled{job="{{.Release.Name}}-kube-state"}
              -
            kube_daemonset_status_current_number_scheduled{job="{{.Release.Name}}-kube-state"} > 0
          for: 10m
          labels:
            severity: critical
            tier: platform
        - alert: KubeDaemonSetMisScheduled
          annotations:
            description: '{{`{{ $value }}`}} Pods of DaemonSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset
              }}`}} are running where they are not supposed to run.'
            runbook_url:
            summary: Daemonset pods are scheduled on wrong nodes
          expr: |
            kube_daemonset_status_number_misscheduled{job="{{.Release.Name}}-kube-state"} > 0
          for: 10m
          labels:
            severity: warning
            tier: platform
        - alert: KubeCronJobRunning
          annotations:
            description: CronJob {{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}} is taking more
              than 1h to complete.
            runbook_url:
            summary: Cronjob is taking a long time to complete
          expr: |
            time() - kube_cronjob_next_schedule_time{job="{{.Release.Name}}-kube-state"} > 3600
          for: 1h
          labels:
            severity: warning
            tier: platform
        - alert: KubeJobCompletion
          annotations:
            description: Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} is taking more
              than one hour to complete.
            runbook_url:
            summary: Cronjob took a look time to complete
          expr: |
            kube_job_spec_completions{job="{{.Release.Name}}-kube-state"} - kube_job_status_succeeded{job="{{.Release.Name}}-kube-state"}  > 0
          for: 1h
          labels:
            severity: warning
            tier: platform
        - alert: KubeJobFailed
          annotations:
            description: Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} failed to complete.
            runbook_url:
            summary: Kube Job failed
          expr: |
            kube_job_failed{job="{{.Release.Name}}-kube-state"}  > 0
          for: 15m
          labels:
            severity: warning
            tier: platform
      - name: kubernetes-resources
        rules:
        - alert: KubeCPUOvercommit
          annotations:
            description: Cluster has overcommitted CPU resource requests for Pods and cannot
              tolerate node failure.
            runbook_url:
            summary: Cluster CPU resources are overcommitted
          expr: |
            sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum)
              /
            sum(kube_node_status_allocatable_cpu_cores)
              >
            (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
          for: 5m
          labels:
            severity: warning
            tier: platform
        - alert: KubeMemOvercommit
          annotations:
            description: Cluster has overcommitted memory resource requests for Pods and cannot
              tolerate node failure.
            runbook_url:
            summary: Cluster Memory resources are overcommitted
          expr: |
            sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
              /
            sum(kube_node_status_allocatable_memory_bytes)
              >
            (count(kube_node_status_allocatable_memory_bytes)-1)
              /
            count(kube_node_status_allocatable_memory_bytes)
          for: 5m
          labels:
            severity: warning
            tier: platform
        - alert: KubeCPUOvercommit
          annotations:
            description: Cluster has overcommitted CPU resource requests for Namespaces.
            runbook_url:
            summary: Cluster has overcommitted CPU resource requests for Namespaces.
          expr: |
            sum(kube_resourcequota{job="{{.Release.Name}}-kube-state", type="hard", resource="cpu"})
              /
            sum(kube_node_status_allocatable_cpu_cores)
              > 1.5
          for: 5m
          labels:
            severity: warning
            tier: platform
        - alert: KubeMemOvercommit
          annotations:
            description: Cluster has overcommitted memory resource requests for Namespaces.
            runbook_url:
            summary: Cluster has overcommitted Memory resource requests for Namespaces.
          expr: |
            sum(kube_resourcequota{job="{{.Release.Name}}-kube-state", type="hard", resource="memory"})
              /
            sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"})
              > 1.5
          for: 5m
          labels:
            severity: warning
            tier: platform
        - alert: KubeQuotaExceeded
          annotations:
            description: Namespace {{`{{ $labels.namespace }}`}} is using {{`{{ $value | humanizePercentage
              }}`}} of its {{`{{ $labels.resource }}`}} quota.
            runbook_url:
            summary: '{{`{{ $labels.namespace }}`}} Kube Quota Exceeded'
          expr: |
            kube_resourcequota{job="{{.Release.Name}}-kube-state", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{job="{{.Release.Name}}-kube-state", type="hard"} > 0)
              > 0.90
          for: 15m
          labels:
            severity: warning
            tier: platform
        - alert: CPUThrottlingHigh
          annotations:
            description: '{{`{{ $value | humanizePercentage }}`}} throttling of CPU in namespace
              {{`{{ $labels.namespace }}`}} for container {{`{{ $labels.container }}`}} in pod {{`{{
              $labels.pod }}`}}.'
            summary: 'CPU Throttling high in pod {{`{{$labels.pod }}`}}'
            runbook_url:
          expr: |
            sum(increase(container_cpu_cfs_throttled_periods_total{container!="", namespace!~"{{ .Release.Namespace }}", namespace != "kube-system"}[5m])) by (container, pod, namespace)
              /
            sum(increase(container_cpu_cfs_periods_total{container!="", namespace!~"{{ .Release.Namespace }}", namespace != "kube-system"}[5m])) by (container, pod, namespace)
              > ( 51 / 100 )
          for: 15m
          labels:
            silence: cre
            severity: warning
            #tier: platform

        - alert: ContainerMemoryNearTheLimit
          expr: |
            (container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container_name!="POD"} /
            container_spec_memory_limit_bytes{namespace="{{ .Release.Namespace }}", container_name!="POD"}) * 100 > 95 < +Inf
          for: 1h
          labels:
            tier: platform
            component: container
            severity: warning
          annotations:
            summary: "A container in the platform namespace uses more than 95% of the memory limit"
            description: {{ printf "%q" "{{ $labels.container }} in {{ $labels.pod }} is using {{ printf \"%.2f\" $value }}% of available memory." }}

      - name: kubernetes-storage
        rules:
        - alert: KubePersistentVolumeUsageCritical
          annotations:
            description: The PersistentVolume claimed by {{`{{ $labels.persistentvolumeclaim
              }}`}} in Namespace {{`{{ $labels.namespace }}`}} is only {{`{{ $value | humanizePercentage
              }}`}} free.
            runbook_url:
            summary: '{{`{{ $labels.persistentvolumeclaim}}`}} Is running out of storage capacity'
          expr: |
            kubelet_volume_stats_available_bytes{job="kubernetes-nodes"}
              /
            kubelet_volume_stats_capacity_bytes{job="kubernetes-nodes"}
              < 0.03
          for: 1m
          labels:
            severity: critical
            tier: platform
        - alert: KubePersistentVolumeFullInFourDays
          annotations:
            description: Based on recent sampling, the PersistentVolume claimed by {{`{{ $labels.persistentvolumeclaim
              }}`}} in Namespace {{`{{ $labels.namespace }}`}} is expected to fill up within four
              days. Currently {{`{{ $value | humanizePercentage }}`}} is available.
            runbook_url:
            summary: '{{`{{ $labels.persistentvolumeclaim}}`}} Will fill up in 4 days'
          expr: |
            (
              kubelet_volume_stats_available_bytes{job="kubernetes-nodes"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubernetes-nodes"}
            ) < 0.15
            and
            predict_linear(kubelet_volume_stats_available_bytes{job="kubernetes-nodes"}[6h], 4 * 24 * 3600) < 0
          for: 1h
          labels:
            severity: high
            tier: platform
        - alert: KubePersistentVolumeErrors
          annotations:
            description: The persistent volume {{`{{ $labels.persistentvolume }}`}} has status {{`{{
              $labels.phase }}`}}.
            runbook_url:
            summary: 'Persistent Volume {{`{{ $labels.persistentvolume }}`}} Is not ready'
          expr: |
            kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="{{.Release.Name}}-kube-state"} > 0
          for: 5m
          labels:
            severity: critical
            tier: platform
      - name: kubernetes-system
        rules:
        - alert: KubeVersionMismatch
          annotations:
            description: There are {{`{{ $value }}`}} different semantic versions of Kubernetes
              components running.
            runbook_url:
            summary: Different versions of kubernetes detected across nodes
          expr: |
            count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
          for: 15m
          labels:
            severity: warning
            tier: platform

        - alert: KubeNamespaceStuckAtTerminating
          expr: |
            kube_namespace_status_phase{phase="Terminating"} == 1
          for: 1h
          labels:
            tier: platform
            component: namespace
          annotations:
            summary: "Namespace in TERMINATING state for more than 1 hour"
            description: {{ printf "%q" "Namespace {{ $labels.namespace }} has been in a TERMINATING state for longer than an hour." }}

      - name: kubernetes-system-kubelet
        rules:
        - alert: KubeNodeNotReady
          annotations:
            description: '{{`{{ $labels.node }}`}} has been unready for more than 15 minutes.'
            runbook_url:
            summary: 'Node {{`{{ $labels.node }}`}} is not ready'
          expr: |
            kube_node_status_condition{job="{{.Release.Name}}-kube-state",condition="Ready",status="true"} == 0
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeNodeUnreachable
          annotations:
            description: '{{`{{ $labels.node }}`}} is unreachable and some workloads may be rescheduled.'
            runbook_url:
            summary: '{{`{{ $labels.node }}`}} is unreachable'
          expr: |
            kube_node_spec_taint{job="{{.Release.Name}}-kube-state",key="node.kubernetes.io/unreachable",effect="NoSchedule"} == 1
          for: 2m
          labels:
            severity: high
            tier: platform
        - alert: KubeletTooManyPods
          annotations:
            description: Kubelet '{{`{{ $labels.node }}`}}' is running at {{`{{ $value | humanizePercentage
              }}`}} of its Pod capacity.
            runbook_url:
            summary: Too many pods on '{{`{{ $labels.node }}`}}'
          expr: |
            count by(node) ( (kube_pod_status_phase{job="{{.Release.Name}}-kube-state",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="{{.Release.Name}}-kube-state"}) ) / max by(node) ( kube_node_status_capacity_pods{job="{{.Release.Name}}-kube-state"} != 1 ) > 0.95
          for: 15m
          labels:
            severity: warning
            tier: platform
      - name: prometheus
        rules:
        - alert: PrometheusBadConfig
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has failed to
              reload its configuration.
            summary: Failed Prometheus configuration reload.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(prometheus_config_last_reload_successful{job="prometheus-k8s",namespace="monitoring"}[5m]) == 0
          for: 10m
          labels:
            severity: critical
            tier: platform
        - alert: PrometheusNotificationQueueRunningFull
          annotations:
            description: Alert notification queue of Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}}
              is running full.
            summary: Prometheus alert notification queue predicted to run full in less
              than 30m.
          expr: |
            # Without min_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            (
              predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s",namespace="monitoring"}[5m], 60 * 30)
            >
              min_over_time(prometheus_notifications_queue_capacity{job="prometheus-k8s",namespace="monitoring"}[5m])
            )
          for: 15m
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
          annotations:
            description: '{{`{{ printf "%.1f" $value }}`}}% errors while sending alerts from
              Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} to Alertmanager {{`{{$labels.alertmanager}}`}}.'
            summary: Prometheus has encountered more than 1% errors sending alerts to
              a specific Alertmanager.
          expr: |
            (
              rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
            /
              rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
            )
            * 100
            > 1
          for: 15m
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
          annotations:
            description: '{{`{{ printf "%.1f" $value }}`}}% minimum errors while sending alerts
              from Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} to any Alertmanager.'
            summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
          expr: |
            min without(alertmanager) (
              rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
            /
              rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
            )
            * 100
            > 3
          for: 15m
          labels:
            severity: critical
            tier: platform
        - alert: PrometheusNotConnectedToAlertmanagers
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is not connected
              to any Alertmanagers.
            summary: Prometheus is not connected to any Alertmanagers.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s",namespace="monitoring"}[5m]) < 1
          for: 10m
          labels:
            severity: high
            tier: platform
        - alert: PrometheusTSDBReloadsFailing
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has detected
              {{`{{$value | humanize}}`}} reload failures over the last 3h.
            summary: Prometheus has issues reloading blocks from disk.
          expr: |
            increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s",namespace="monitoring"}[3h]) > 0
          for: 4h
          labels:
            severity: high
            tier: platform
        - alert: PrometheusTSDBCompactionsFailing
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has detected
              {{`{{$value | humanize}}`}} compaction failures over the last 3h.
            summary: Prometheus has issues compacting blocks.
          expr: |
            increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s",namespace="monitoring"}[3h]) > 0
          for: 4h
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusNotIngestingSamples
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is not ingesting
              samples.
            summary: Prometheus is not ingesting samples.
          expr: |
            rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s",namespace="monitoring"}[5m]) <= 0
          for: 10m
          labels:
            severity: high
            tier: platform
        - alert: PrometheusDuplicateTimestamps
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is dropping
              {{`{{ printf "%.4g" $value  }}`}} samples/s with different values but duplicated
              timestamp.
            summary: Prometheus is dropping samples with duplicate timestamps.
          expr: |
            rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
          for: 10m
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusOutOfOrderTimestamps
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is dropping
              {{`{{ printf "%.4g" $value  }}`}} samples/s with timestamps arriving out of order.
            summary: Prometheus drops samples with out-of-order timestamps.
          expr: |
            rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
          for: 10m
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusRuleFailures
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has failed to
              evaluate {{`{{ printf "%.0f" $value }}`}} rules in the last 5m.
            summary: Prometheus is failing rule evaluations.
          expr: |
            increase(prometheus_rule_evaluation_failures_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: PrometheusMissingRuleEvaluations
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has missed {{`{{
              printf "%.0f" $value }}`}} rule group evaluations in the last 5m.
            summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
          expr: |
            increase(prometheus_rule_group_iterations_missed_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
            tier: platform
      - name: alertmanager.rules
        rules:
        - alert: AlertmanagerConfigInconsistent
          annotations:
            description: The configuration of the instances of the Alertmanager cluster {{`{{$labels.service}}`}}
              are out of sync.
            summary: Alert manager config is out if sync
          expr: |
            count_values("config_hash", alertmanager_config_hash{job="alertmanager-main",namespace="monitoring"}) BY (service) / ON(service) GROUP_LEFT() label_replace(max(prometheus_operator_spec_replicas{job="prometheus-operator",namespace="monitoring",controller="alertmanager"}) by (name, job, namespace, controller), "service", "alertmanager-$1", "name", "(.*)") != 1
          for: 5m
          labels:
            severity: high
            tier: platform
        - alert: AlertmanagerFailedReload
          annotations:
            description: Reloading Alertmanager's configuration has failed for {{`{{ $labels.namespace
              }}`}}/{{`{{ $labels.pod}}`}}.
            summary: Alertmanager config reload failed
          expr: |
            alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="monitoring"} == 0
          for: 10m
          labels:
            severity: high
            tier: platform
        - alert: AlertmanagerMembersInconsistent
          annotations:
            description: Alertmanager has not found all other members of the cluster.
            summary: Alertmanager has not found all other members of the cluster.
          expr: |
            alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"}
              != on (service) GROUP_LEFT()
            count by (service) (alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"})
          for: 5m
          labels:
            severity: critical
            tier: platform
      - name: general.rules
        rules:
        - alert: TargetDown
          annotations:
            description: '{{`{{ printf "%.4g" $value }}`}} % of the {{`{{ $labels.job }}`}}/{{`{{ $labels.service
              }}`}} targets in {{`{{ $labels.namespace }}`}} namespace are down.'
            summary: '{{`{{ $labels.job }}`}} Target Down'
          expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job,namespace, service)) > 10
          for: 10m
          labels:
            severity: critical
            tier: platform

        - alert: NodeDown
          annotations:
            runbook_url:
            description: '{{`{{ $labels.instance }}`}} is not reachable. Prometheus could not contact node-exporter on this node. This is usually a sign of an issue on the node.'
            summary: '{{`{{ $labels.instance }}`}} is not reachable'
          expr: up{job="node-exporter"} == 0
          for: 10m
          labels:
            severity: critical
            tier: platform

        - alert: Watchdog
          annotations:
            description: |
              This is an alert meant to ensure that the entire alerting pipeline is functional.
              This alert is always firing, therefore it should always be firing in Alertmanager
              and always fire against a receiver. There are integrations with various notification
              mechanisms that send a notification when this alert is not firing. For example the
              "DeadMansSnitch" integration in PagerDuty.
          expr: vector(1)
          labels:
            severity: none
            tier: none
      - name: node-time
        rules:
        - alert: ClockSkewDetected
          annotations:
            description: Clock skew detected on node-exporter {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod
              }}`}}`. Ensure NTP is configured correctly on this host.
            summary: Time is slipping on a node
          expr: |
            abs(node_timex_offset_seconds{job="node-exporter"}) > 0.05
          for: 2m
          labels:
            severity: high
            tier: platform
      - name: node-network
        rules:
        - alert: NodeNetworkInterfaceFlapping
          annotations:
            description: Network interface "{{`{{ $labels.device }}`}}" changing it's up status
              often on node-exporter {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}}"
            summary: Network Interface Flapping
          expr: |
            changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
          for: 2m
          labels:
            severity: warning
            tier: platform


      {{- if .Values.global.veleroEnabled }}
      - name: velero
        rules:
        - alert: VeleroMetricsAbsent
          expr: absent(up{job="velero"} == 1)
          for: 15m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "Velero has disappeared from Prometheus target discovery"
            description: "This alert fires if Prometheus target discovery was not able to
              scrape Velero metrics in the last 15 minutes."

        - alert: FailedVeleroBackup
          expr: increase(velero_backup_failure_total{job="velero"}[1d]) > 0
          for: 5m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "Velero failed to backup"
            description: {{ printf "%q" "Backup attempt failed in Velero past 1 day \n LABELS - {{ $labels }}." }}

        - alert: PartialFailedVeleroBackup
          expr: increase(velero_backup_partial_failure_total{job="velero"}[1d]) > 0
          for: 5m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "Velero partially failed to backup"
            description: {{ printf "%q" "Backup attempt partially failed in Velero past 1 day \n LABELS - {{ $labels }}." }}

        - alert: VeleroNoBackupForLong
          expr: time() - velero_backup_last_successful_timestamp > 172800
          for: 5m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "2 days since last successful Backup for a Velero Schedule"
            description: {{ printf "%q" "Time since last successful backup for {{ $labels.schedule }} :\n  VALUE = {{ $value }}\n  LABELS - {{ $labels }}" }}
        {{- end }}

      - name: Ingress
        rules:
        - alert: NginxIngressHigh4XXRate
          expr: >-
            sum by (ingress) (rate(nginx_ingress_controller_requests{status=~"^4..", namespace="{{.Release.Namespace}}"}[5m])) / sum by (ingress) (rate(nginx_ingress_controller_requests{ingress!="", namespace="{{.Release.Namespace}}"}[5m])) * 100 > 5
            and
            sum by (ingress) (rate(nginx_ingress_controller_requests{status=~"^4..", namespace="astronomer"}[5m])) > 5
          for: 10m
          labels:
            tier: platform
            component: nginx
          annotations:
            summary: {{ printf "%q" "NGINX Ingress {{ $labels.ingress }} failure rate is {{ printf \"%.2f\" $value }}%." }}
            description: "This alert fires if the failure rate (the rate of 4xx responses)
              measured on a time window of 5 minutes was higher than 5% of all traffic
              AND more than 5 per minute for the last 10 minutes."

        - alert: NginxIngressHigh5XXRate
          expr: >-
            sum by (ingress) (rate(nginx_ingress_controller_requests{status=~"^5..",  namespace="{{.Release.Namespace}}"}[5m])) / sum by (ingress) (rate(nginx_ingress_controller_requests{ingress!="", namespace="{{.Release.Namespace}}"}[5m])) * 100 > 5
            and
            sum by (ingress) (rate(nginx_ingress_controller_requests{status=~"^5..", namespace="astronomer"}[5m])) > 5
          for: 5m
          labels:
            tier: platform
            component: nginx
          annotations:
            summary: {{ printf "%q" "NGINX Ingress {{ $labels.ingress }} failure rate is {{ printf \"%.2f\" $value }}%." }}
            description: "This alert fires if the failure rate (the rate of 5xx responses)
              measured on a time window of 5 minutes was higher than 5% of all traffic
              AND more than 5 per minute for the last 10 minutes."

      - name: logging
        rules:
        - alert: FluentdQueueLarge
          expr: sum by (host)( fluentd_status_buffer_queue_length) > 2
          for: 15m
          labels:
            tier: platform
            component: fluentd
            severity: critical
          annotations:
            summary: {{ printf "%q" "Fluentd buffer queue length is {{ $value }} For {{ $labels.host }} " }}
            description: "Check on Elastic Search client and try restarting Fluentd pods"

        - alert: ElasticSeachUnassignedShards
          expr: elasticsearch_cluster_health_unassigned_shards > 0
          for: 10m
          labels:
            severity: high
            tier: platform
          annotations:
            summary: {{ printf "%q" "Elastic Search has {{ $value }} unassigned shards" }}
            description: "Unassigned shards in Elastic Search cluster"

        # There are two important thresholds that impact how ES node allocates index shards.
        #   - 85% used - Low watermark
        #   - 90% used - High watermark
        #   - 95% used - Flood watermark
        #
        # It is important the make sure there is enough free disk space for automatic background Lucene segment merges.
        # Ideally as much free disk space as total sum of actual segment size (ie, data can fit into disk twice).
        #
        # https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#disk-based-shard-allocation

        - alert: ElasticDiskLowWatermarkReached

          expr: sum by (name) (
                    round(
                      (1 - (
                        elasticsearch_filesystem_data_available_bytes /
                        elasticsearch_filesystem_data_size_bytes
                      )
                    ) * 100, 0.001)
                  ) > 85
          for: 5m
          labels:
            severity: warning
            tier: platform
          annotations:
            summary: {{"Low Watermark Reached - disk saturation is {{ $value }}%"}}
            description: {{"Low Watermark Reached at {{ $labels.node }} Elasticsearch will not allocate shards to node"}}

        - alert: ElasticDiskHighWatermarkReached
          expr: sum by (name) (
                    round(
                      (1 - (
                        elasticsearch_filesystem_data_available_bytes /
                        elasticsearch_filesystem_data_size_bytes
                      )
                    ) * 100, 0.001)
                  ) > 90
          for: 5m
          labels:
            severity: high
            tier: platform
          annotations:
            summary: {{"High Watermark Reached - disk saturation is {{ $value }}%"}}
            description: {{"High Watermark Reached at {{ $labels.node }} Elasticsearch will attempt to relocate shards away from a node"}}

        - alert: ElasticDiskFloodWatermarkReached
          expr: sum by (name) (
                    round(
                      (1 - (
                        elasticsearch_filesystem_data_available_bytes /
                        elasticsearch_filesystem_data_size_bytes
                      )
                    ) * 100, 0.001)
                  ) > 95
          for: 5m
          labels:
            severity: critical
            tier: platform
          annotations:
            summary: {{"Flood Watermark Reached - disk saturation is {{ $value }}%"}}
            description: {{"Flood Watermark Reached at {{ $labels.node }} Elasticsearch enforcing a read-only index block"}}

        - alert: IngessCertificateExpiration
          expr: avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time() < 604800
          for: 10s
          labels:
            severity: high
            tier: platform
            component: ingress
          annotations:
            summary: "TLS Certificate Expiring Soon"
            {{/* We want '{{ $labels.host }}' to be evaluated by prometheus, not helm, so we have to escape it once */ -}}
            description: {{ printf "%q" "The TLS Certificate for {{ $labels.host }} is expiring in less than a week." }}

      {{- if .Values.global.prometheusPostgresExporterEnabled }}
      - name: PostgreSQL
        rules:
        - alert: AirflowChartVersionInconsistent
          expr: airflow_chart_version_unconverged > 0
          for: 5m
          labels:
            severity: warning
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "{{ $value }} rows in the 'version' column of the 'Deployments' table do not match the rest" }}
            description: "This alert is used to detect if all Airflow deployments have the same 'version' configuration in the database"

        - alert: PostgreSQLMaxConnectionsReached
          expr: sum(pg_stat_activity_count) by (instance) >= sum(pg_settings_max_connections) by (instance) - sum(pg_settings_superuser_reserved_connections) by (instance)
          for: 1m
          labels:
            severity: critical
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "{{ $labels.instance }} has maxed out Postgres connections." }}
            description: {{ printf "%q" "{{ $labels.instance }} is exceeding the currently configured maximum Postgres connection limit (current value: {{ $value }}s). Services may be degraded - please take immediate action (you probably need to increase max_connections in the Docker image and re-deploy." }}

        - alert: PostgreSQLHighConnections
          expr: sum(pg_stat_activity_count) by (instance) > (sum(pg_settings_max_connections) by (instance) - sum(pg_settings_superuser_reserved_connections) by (instance)) * 0.8
          for: 10m
          labels:
            severity: critical
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "{{ $labels.instance }} is over 80% of max Postgres connections." }}
            description: {{ printf "%q" "{{ $labels.instance }} is exceeding 80% of the currently configured maximum Postgres connection limit (current value: {{ $value }}s). Please check utilization graphs and confirm if this is normal service growth, abuse or an otherwise temporary condition or if new resources need to be provisioned (or the limits increased, which is mostly likely)." }}

        - alert: PostgreSQLDown
          expr: pg_up != 1
          for: 1m
          labels:
            severity: critical
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "PostgreSQL is not processing queries: {{ $labels.instance }}" }}
            description: {{ printf "%q" "{{ $labels.instance }} is rejecting query requests from the exporter, and thus probably not allowing DNS requests to work either. User services should not be effected provided at least 1 node is still alive." }}

        - alert: PostgreSQLSlowQueries
          expr: avg(rate(pg_stat_activity_max_tx_duration{datname!~"template.*"}[2m])) by (datname) > 2 * 60
          for: 2m
          labels:
            severity: warning
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "PostgreSQL high number of slow on {{ $labels.cluster }} for database {{ $labels.datname }} " }}
            description: {{ printf "%q" "PostgreSQL high number of slow queries {{ $labels.cluster }} for database {{ $labels.datname }} with a value of {{ $value }} " }}
      {{- end }}
