################################
## Prometheus Alerts ConfigMap
#################################
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ template "prometheus.fullname" . }}-alerts
  labels:
    tier: monitoring
    component: {{ template "prometheus.name" . }}
    chart: {{ template "prometheus.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  alerts.yaml: |-
    groups:
      - name: airflow
        rules:
        - alert: AirflowDeploymentUnhealthy
          expr: sum by(release) (kube_pod_container_status_running{namespace=~".*-.*-.*-[0-9]{4}",pod=~".*-.*-[0-9]{4}-.*",container=~".*(scheduler|scheduler-gc|webserver|worker|statsd|pgbouncer|metrics-exporter|redis|flower)"}) - count by(release) (kube_pod_container_status_running{namespace=~".*-.*-.*-[0-9]{4}",pod=~".*-.*-[0-9]{4}-.*",container=~".*(scheduler|scheduler-gc|webserver|worker|statsd|pgbouncer|metrics-exporter|redis|flower)"}) < 0
          for: 15m # Rough number but should be enough to clear deployments with a reasonable amount of workers
          labels:
            tier: airflow
            component: deployment
            deployment: {{ printf "%q" "{{ $labels.release }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.release }} deployment is unhealthy" }}
            description: {{ printf "%q" "The {{ $labels.release }} deployment is not completely available." }}


        # This alert depends on the scheduler_heartbeat metric being a counter.
        # The type filter here was introduced in 0.7.0, so we don't trigger alerts for
        # older deployments.
        - alert: AirflowSchedulerUnhealthy
          expr: rate(airflow_scheduler_heartbeat{type="counter"}[1m]) == 0
          for: 6m
          labels:
            tier: airflow
            component: scheduler
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.deployment }} scheduler is unhealthy" }}
            description: {{ printf "%q" "The {{ $labels.deployment }} scheduler's heartbeat has dropped below the acceptable rate." }}

        - alert: AirflowPodQuota
          expr: (sum by (release) (kube_resourcequota{resource="pods", type="used", namespace=~".*-.*-.*-[0-9]{4}"}) / sum by (release) (kube_resourcequota{resource="pods", type="hard", namespace=~".*-.*-.*-[0-9]{4}"})*100) > 95
          for: 10m
          labels:
            tier: airflow
            component: deployment
            deployment: {{ printf "%q" "{{ $labels.release }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.release }} is near its pod quota" }}
            description: {{ printf "%q" "{{ $labels.release }} has been using over 95% of its pod quota for over 10 minutes." }}

        - alert: AirflowEphemeralStorageLimit
          expr: (container_fs_usage_bytes{pod_name=~".*(scheduler|webserver|worker).*"}) >= 1800000000
          for: 5m
          labels:
            tier: airflow
            component: {{ printf "%q" "{{ $labels.component_name }}" }}
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.deployment }} {{ $labels.component_name }} is near its ephemeral storage limit" }}
            description: {{ printf "%q" "{{ $labels.deployment }} {{ $labels.component_name }} has been using 90% of its ephemeral storage for over 5 minutes." }}

        - alert: AirflowTasksPendingIncreasing
          expr: (max_over_time(airflow_scheduler_tasks_pending[5m]) - max_over_time(airflow_scheduler_tasks_pending[5m] offset 5m)) > 0
          for: 30m
          labels:
            tier: airflow
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.deployment }} is creating tasks faster than it's clearing them." }}
            description: {{ printf "%q" "{{ $labels.deployment }}: the number of pending tasks has been increasing for 30 minutes" }}

        - alert: CpuThrottlingInDeployment
          expr: 100 * (increase(container_cpu_cfs_throttled_periods_total{namespace!="{{ .Release.Name }}", container_name!~".*worker.*|()"}[5m]) / increase(container_cpu_cfs_periods_total{namespace!="{{ .Release.Name }}", container_name!~".*worker.*|()"}[5m])) > 75
          for: 5m
          labels:
            tier: airflow
            severity: warning
          annotations:
            summary: {{ printf "%q" "{{ $labels.pod_name }} ({{ $labels.container_name }}) in namespace {{ $labels.namespace }} is getting throttled {{ $value }}% of the time" }}
            description: "In the past 5 minutes, one or more components in the deployment are experiencing CPU throttling."

      - name: platform
        rules:

        - alert: AirflowOperatorFailureRate
          expr: 100 * (sum by (operator) (increase(airflow_operator_failures{operator=~"([A-Z][a-z0-9]+)(([0-9])|([A-Z0-9][a-z0-9]+))*([A-Z])?Operator"}[1h])) / (sum by (operator) (increase(airflow_operator_successes{operator=~"([A-Z][a-z0-9]+)(([0-9])|([A-Z0-9][a-z0-9]+))*([A-Z])?Operator"}[1h])) + sum by (operator) (increase(airflow_operator_failures{operator=~"([A-Z][a-z0-9]+)(([0-9])|([A-Z0-9][a-z0-9]+))*([A-Z])?Operator"}[1h])))) > 50
          for: 2h
          labels:
            tier: platform
            severity: info
            operator: {{ printf "%q" "{{ $labels.operator }}" }}
          annotations:
            summary: {{ printf "%q" "Across all deployments, {{ $labels.operator }} is failing {{ $value }}% of the time" }}
            description: "This alarm is used to notify support if an external data source may be inoperational, but the query is subject to be incorrect for small sample sizes where a single deployment uses an operator more than the rest of deployments combined over the course of two hours"

        - alert: CpuThrottlingWarning
          expr: 100 * (increase(container_cpu_cfs_throttled_periods_total{namespace="{{ .Release.Name }}", container_name=~".{1,1000}"}[5m]) / increase(container_cpu_cfs_periods_total{namespace="{{ .Release.Name }}", container_name=~".{1,1000}"}[5m])) > 50
          for: 5m
          labels:
            tier: platform
            severity: warning
          annotations:
            summary: {{ printf "%q" "{{ $labels.pod_name }} ({{ $labels.container_name }}) is getting throttled {{ $value }}% of the time, should check next business day" }}
            description: "Half or more of the time in the past 5 minutes, one or more platform components are experiencing some CPU throttling."

        - alert: CpuThrottlingSevere
          expr: 100 * (increase(container_cpu_cfs_throttled_periods_total{namespace="{{ .Release.Name }}", container_name=~".{1,1000}"}[5m]) / increase(container_cpu_cfs_periods_total{namespace="{{ .Release.Name }}", container_name=~".{1,1000}"}[5m])) > 80
          for: 5m
          labels:
            tier: platform
            severity: urgent
          annotations:
            summary: {{ printf "%q" "{{ $labels.pod_name }} ({{ $labels.container_name }}) is getting throttled {{ $value }}% of the time" }}
            description: "In the past 5 minutes, one or more components in the platform namespace are experiencing throttling 80% or more of the time"
        - alert: FluentdQueueLarge
          expr: sum(fluentd_status_buffer_queue_length) > 10
          for: 5m
          labels:
            tier: platform
            component: fluentd
            severity: urgent
          annotations:
            summary: "Fluentd unwrittten log queue is geting large"
            description: "Check on Elastic Search client and try restarting Fluentd pods"

        - alert: PrometheusDiskUsage
          expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-{{ template "prometheus.fullname" . }}-.*"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-{{ template "prometheus.fullname" . }}-.*"} * 100) < 10
          for: 5m
          labels:
            tier: platform
            component: prometheus
          annotations:
            summary: "Prometheus High Disk Usage"
            description: "Prometheus has less than 10% disk space available."

        - alert: RegistryDiskUsage
          expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-{{ .Release.Name }}-registry-.*"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-{{ .Release.Name }}-registry-.*"} * 100) < 10
          for: 5m
          labels:
            tier: platform
            component: registry
          annotations:
            summary: "Docker Registry High Disk Usage"
            description: "Docker Registry has less than 10% disk space available."

        - alert: ElasticsearchDiskUsage
          expr: (sum(kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-{{ .Release.Name }}-elasticsearch-data-.*"}) / sum(kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-{{ .Release.Name }}-elasticsearch-data-.*"}) * 100) < 10
          for: 5m
          labels:
            tier: platform
            component: elasticsearch
          annotations:
            summary: "Elasticsearch High Disk Usage"
            description: "Elasticsearch has less than 10% disk space available."

        - alert: IngessCertificateExpiration
          expr: avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time() < 604800
          for: 10s
          labels:
            tier: platform
            component: ingress
          annotations:
            summary: "TLS Certificate Expiring Soon"
            {{/* We want '{{ $labels.host }}' to be evaluted by prometheus, not helm, so we have to escape it once */ -}}
            description: {{ printf "%q" "The TLS Certificate for {{ $labels.host }} is expiring in less than a week." }}

        - alert: PrometheusNotConnectedToAlertmanagers
          expr: prometheus_notifications_alertmanagers_discovered{job="prometheus"} < 1
          for: 10m
          labels:
            tier: platform
            component: prometheus
          annotations:
            summary: "Prometheus is not connected to any Alertmanagers"
            description: "Prometheus is not connected to any Alertmanagers"

        - alert: NginxIngressHighFailureRate
          expr: |
            (sum by (ingress) (rate(nginx_ingress_controller_requests{ingress!="", status=~"[4-5][0-9][0-9]"}[2m]))
              /
            sum by (ingress) (rate(nginx_ingress_controller_requests{ingress!=""}[2m]))) * 100 > 10
          for: 10m
          labels:
            tier: platform
            component: nginx
          annotations:
            summary: {{ printf "%q" "NGINX Ingress {{ $labels.ingress }} failure rate is {{ printf \"%.2f\" $value }}%." }}
            description: "This alert fires if the failure rate (the rate of 4xx or 5xx reponses)
              measured on a time window of 2 minutes was higher than 10% in the last
              10 minutes."

        - alert: KubeNamespaceStuckAtTerminating
          expr: |
            kube_namespace_status_phase{namespace=~".*-.*-.*-[0-9]{4}", phase="Terminating"} == 1
          for: 1h
          labels:
            tier: platform
            component: namespace
          annotations:
            summary: "Namespace in TERMINATING state for more than 1 hour"
            description: {{ printf "%q" "Namespace {{ $labels.namespace }} has been in a TERMINATING state for longer than an hour." }}

        - alert: ContainerMemoryAtTheLimit
          expr: |
            container_memory_working_set_bytes{container_name!="POD"} /
            container_spec_memory_limit_bytes{container_name!="POD"} > 0.8 < +Inf
          for: 1h
          labels:
            tier: platform
            component: container
          annotations:
            summary: "Container uses more than 80% of the memory limit"
            description: {{ printf "%q" "Memory usage for the {{ $labels.container }} Container in the {{ $labels.pod }} Pod is almost at the limit" }}

        - alert: PlatformContainerRestarting
          expr: |
            rate(kube_pod_container_status_restarts_total{namespace="astronomer", container!="curator"}[90s]) * 60 > 0
          labels:
            tier: platform
            severity: warning
          annotations:
            summary: "A container has restarted in the Astronomer platform namespace"
            description: {{ printf "%q" "The container {{ $labels.container }} in pod {{ $labels.pod }} has restarted at least once" }}

        - alert: PlatformContainerRestartingContinuously
          expr: |
            rate(kube_pod_container_status_restarts_total{namespace="astronomer", pod!~".*elasticsearch-curator.*"}[600s]) * 60 > 0
          for: 10m
          labels:
            tier: platform
            severity: urgent
          annotations:
            summary: "A container in the Astronomer platform namespace is stuck crash looping"
            description: {{ printf "%q" "The container {{ $labels.container }} in pod {{ $labels.pod }} is stuck crash looping" }}

        - alert: KubePodNotReady
          expr: |
            sum by (namespace, pod) (kube_pod_status_phase{job="kube-state", phase!~"Running|Succeeded"}) > 0
          for: 15m
          labels:
            tier: platform
            component: pod
          annotations:
            summary: "Pod in non-ready state for more than 15 minutes"
            description: {{ printf "%q" "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes." }}

        - alert: NginxIngressDown
          expr: |
            absent(up{job="nginx"} == 1)
          for: 15m
          labels:
            tier: platform
            component: nginx
          annotations:
            summary: "Nginx Ingress Controller has disappeared from Prometheus target discovery"
            description: "This alert fires if Prometheus target discovery was not able to
              reach ingress-nginx-metrics in the last 15 minutes."

        {{- if .Values.global.veleroEnabled }}
        - alert: VeleroMetricsAbsent
          expr: absent(up{job="velero"} == 1)
          for: 15m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "Velero has disappeared from Prometheus target discovery"
            description: "This alert fires if Prometheus target discovery was not able to
              scrape Velero metrics in the last 15 minutes."

        - alert: FailedVeleroBackup
          expr: increase(velero_backup_failure_total{job="velero"}[1d]) > 0
          for: 5m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "Velero failed to backup"
            description: {{ printf "%q" "Backup attempt failed in Velero past 1 day \n LABELS - {{ $labels }}." }}

        - alert: PartialFailedVeleroBackup
          expr: increase(velero_backup_partial_failure_total{job="velero"}[1d]) > 0
          for: 5m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "Velero partially failed to backup"
            description: {{ printf "%q" "Backup attempt partially failed in Velero past 1 day \n LABELS - {{ $labels }}." }}

        - alert: VeleroNoBackupForLong
          expr: time() - velero_backup_last_successful_timestamp > 172800
          for: 5m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "2 days since last successful Backup for a Velero Schedule"
            description: {{ printf "%q" "Time since last successful backup for {{ $labels.schedule }} :\n  VALUE = {{ $value }}\n  LABELS - {{ $labels }}" }}

        - alert: SchedulersNotHealthy
          expr: (count(rate(airflow_scheduler_heartbeat{}[1m]) > 0) / count(rate(airflow_scheduler_heartbeat{}[1m]))) < 0.5
          for: 5m
          labels:
            tier: platform
            component: airflow
            severity: urgent
          annotations:
            summary: "Half or more of schedulers do not have a heartbeat"
            description: "Half or more of schedulers do not have a heartbeat in the last five minutes"

        {{- end }}
