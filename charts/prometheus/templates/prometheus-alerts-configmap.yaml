#################################
## Prometheus Alerts ConfigMap ##
#################################

## Severity Definitions
# Info - goes to an info channel specific to the group that is interested. This should be very little
# Warning - goes to warning channel
#   things we want to know about but are not always actionable. Get a pulse of potential future issues
# High - Goes to slack prod alerts channel
#   Things that are actionable and should be taken care of but do not need to wake someone up
# Critical - goes to slack prod alerts channel and pager duty
# Things that would impact uptime or customer experience and are worth waking up in the middle of the night

kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ template "prometheus.fullname" . }}-alerts
  labels:
    tier: monitoring
    component: {{ template "prometheus.name" . }}
    chart: {{ template "prometheus.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  alerts: |-
    groups:
      - name: airflow
        rules:
        {{- if and (not .Values.additionalAlerts.airflow) (not .Values.defaultAlerts.airflow.enabled) }}
          []
        {{- end }}
        {{- if .Values.additionalAlerts.airflow }}
        {{- tpl .Values.additionalAlerts.airflow $ | nindent 8 }}
        {{- end }}
        {{- if .Values.defaultAlerts.airflow.enabled }}
        - alert: AirflowDeploymentUnhealthy
          expr: sum by(release) (kube_pod_container_status_running{container=~".*(scheduler|scheduler-gc|webserver|api-server|worker|statsd|pgbouncer|metrics-exporter|redis|flower|triggerer)"}) - count by(release) (kube_pod_container_status_running{container=~".*(scheduler|scheduler-gc|webserver|worker|statsd|pgbouncer|metrics-exporter|redis|flower|triggerer)"}) < 0
          for: 15m # Rough number but should be enough to clear deployments with a reasonable amount of workers
          labels:
            tier: airflow
            component: deployment
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            # deployment label isn't populated by the query. However release and deployment are the same thing
            deployment: {{ printf "%q" "{{ $labels.release }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.release }} deployment is unhealthy" }}
            description: {{ printf "%q" "The {{ $labels.release }} deployment is not completely available." }}

        # This alert depends on the scheduler_heartbeat metric being a counter.
        # The type filter here was introduced in 0.7.0, so we don't trigger alerts for
        # older deployments.
        - alert: AirflowSchedulerUnhealthy
          expr: rate(airflow_scheduler_heartbeat{type="counter"}[1m]) == 0
          for: 6m
          labels:
            tier: airflow
            component: scheduler
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.deployment }} scheduler is unhealthy" }}
            description: {{ printf "%q" "The {{ $labels.deployment }} scheduler's heartbeat has dropped below the acceptable rate." }}

        - alert: AirflowPodQuota
          expr: (sum by (release) (kube_resourcequota{resource="pods", type="used"}) / sum by (release) (kube_resourcequota{resource="pods", type="hard"})*100) > 95
          for: 10m
          labels:
            tier: airflow
            component: deployment
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            # deployment label isn't populated by the query. However release and deployment are the same thing
            deployment: {{ printf "%q" "{{ $labels.release }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.release }} is near its pod quota" }}
            description: {{ printf "%q" "{{ $labels.release }} has been using over 95% of its pod quota for over 10 minutes." }}

        - alert: AirflowTasksPendingIncreasing
          expr: (max_over_time(airflow_scheduler_tasks_pending[5m]) - max_over_time(airflow_scheduler_tasks_pending[5m] offset 5m)) > 0
          for: 30m
          labels:
            tier: airflow
            workspace: {{ printf "%q" "{{ $labels.workspace }}" }}
            deployment: {{ printf "%q" "{{ $labels.deployment }}" }}
          annotations:
            summary: {{ printf "%q" "{{ $labels.deployment }} is creating tasks faster than it's clearing them." }}
            description: {{ printf "%q" "{{ $labels.deployment }}: the number of pending tasks has been increasing for 30 minutes" }}
        {{- end }}

      - name: platform
        rules:
        {{- if and (not .Values.additionalAlerts.platform) (not .Values.defaultAlerts.platform.enabled) }}
          []
        {{- end }}
        {{- if .Values.additionalAlerts.platform }}
        {{- tpl .Values.additionalAlerts.platform $ | nindent 8 }}
        {{- end }}
        {{- if .Values.defaultAlerts.platform.enabled }}
        - alert: AirflowOperatorFailureRate
          expr: >-
            100 * (
              sum by (operator) (
                increase(airflow_operator_failures{operator=~"([A-Z][a-z0-9]+)(([0-9])|([A-Z0-9][a-z0-9]+))*([A-Z])?Operator"}[1h])
              ) / (
                sum by (operator) (
                  increase(airflow_operator_successes{operator=~"([A-Z][a-z0-9]+)(([0-9])|([A-Z0-9][a-z0-9]+))*([A-Z])?Operator"}[1h])
                ) +
                sum by (operator) (
                  increase(airflow_operator_failures{operator=~"([A-Z][a-z0-9]+)(([0-9])|([A-Z0-9][a-z0-9]+))*([A-Z])?Operator"}[1h])
                )
              )
            ) > 50
          for: 2h
          labels:
            tier: platform
            severity: warn
            operator: {{ printf "%q" "{{ $labels.operator }}" }}
          annotations:
            summary: {{ printf "%q" "Across all deployments, {{ $labels.operator }} is failing {{ $value }}% of the time" }}
            description: "This alarm is used to notify support if an external data source may be inoperative, but the query is subject to be incorrect for small sample sizes where a single deployment uses an operator more than the rest of deployments combined over the course of two hours"

        - alert: ManyUnhealthySchedulers
          expr: count(rate(airflow_scheduler_heartbeat{}[1m]) <= 0) > 5
          for: 5m
          labels:
            tier: platform
            severity: critical
          annotations:
            summary: {{ printf "%q" "{{ $value }} airflow schedulers are not heartbeating" }}
            description: "If more than 5 Airflow Schedulers are not heartbeating for more than 5 minutes, this alarm fires."

        - alert: SchedulersNotHealthy
          expr: (count(rate(airflow_scheduler_heartbeat{}[1m]) > 0) / count(rate(airflow_scheduler_heartbeat{}[1m]))) < 0.5
          for: 5m
          labels:
            tier: platform
            component: airflow
            severity: critical
          annotations:
            summary: "Half or more of schedulers do not have a heartbeat"
            description: {{ printf "%q" "{{ $value }} }} schedulers do not have a heartbeat in the last five minutes" }}
        {{- end }}

      - name: k8s-recording.rules
        rules:
        - expr: |
            sum by (namespace) (
                sum by (namespace, pod) (
                    max by (namespace, pod, container) (
                        kube_pod_container_resource_requests{job="{{.Release.Name}}-kube-state", resource="memory",unit="byte"}
                    ) * on(namespace, pod) group_left() max by (namespace, pod) (
                        kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace:kube_pod_container_resource_requests_memory_bytes:sum
        - expr: |
            sum by (namespace) (
                sum by (namespace, pod) (
                    max by (namespace, pod, container) (
                        kube_pod_container_resource_requests{job="{{.Release.Name}}-kube-state", resource="cpu", unit="core"}
                    ) * on(namespace, pod) group_left() max by (namespace, pod) (
                      kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace:kube_pod_container_resource_requests_cpu_cores:sum
        - expr: |
            sum(
              label_replace(
                label_replace(
                  kube_pod_owner{job="{{.Release.Name}}-kube-state", owner_kind="ReplicaSet"},
                  "replicaset", "$1", "owner_name", "(.*)"
                ) * on(replicaset, namespace) group_left(owner_name) kube_replicaset_owner{job="{{.Release.Name}}-kube-state"},
                "workload", "$1", "owner_name", "(.*)"
              )
            ) by (cluster, namespace, workload, pod)
          labels:
            workload_type: deployment
          record: mixin_pod_workload
        - expr: |
            sum(
              label_replace(
                kube_pod_owner{job="{{.Release.Name}}-kube-state", owner_kind="DaemonSet"},
                "workload", "$1", "owner_name", "(.*)"
              )
            ) by (cluster, namespace, workload, pod)
          labels:
            workload_type: daemonset
          record: mixin_pod_workload
        - expr: |
            sum(
              label_replace(
                kube_pod_owner{job="{{.Release.Name}}-kube-state", owner_kind="StatefulSet"},
                "workload", "$1", "owner_name", "(.*)"
              )
            ) by (cluster, namespace, workload, pod)
          labels:
            workload_type: statefulset
          record: mixin_pod_workload

      - name: kube-scheduler-recording.rules
        rules:
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile

      - name: kube-prometheus-general-recording.rules
        rules:
        - expr: count without(instance, pod, node) (up == 1)
          record: count:up1
        - expr: count without(instance, pod, node) (up == 0)
          record: count:up0

      - name: kube-state-metrics
        rules:
        - alert: KubeStateMetricsListErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in
              list operations. This is likely causing it to not be able to expose metrics
              about Kubernetes objects correctly or at all.
            summary: Errors with kube-state-metrics list operations
            runbook_url:
          expr: |
            (sum(rate(kube_state_metrics_list_total{job="{{.Release.Name}}-kube-state",result="error"}[5m]))
              /
            sum(rate(kube_state_metrics_list_total{job="{{.Release.Name}}-kube-state"}[5m])))
            > 0.01
          for: 15m
          labels:
            severity: critical
            tier: platform
        - alert: KubeStateMetricsWatchErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in
              watch operations. This is likely causing it to not be able to expose metrics
              about Kubernetes objects correctly or at all.
            runbook_url:
            summary: Errors with kube-state-metrics watch operations
          expr: |
            (sum(rate(kube_state_metrics_watch_total{job="{{.Release.Name}}-kube-state",result="error"}[5m]))
              /
            sum(rate(kube_state_metrics_watch_total{job="{{.Release.Name}}-kube-state"}[5m])))
            > 0.01
          for: 15m
          labels:
            severity: critical
            tier: platform

      - name: kubernetes-apps
        rules:
        - alert: KubePodCrashLooping
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} ({{`{{ $labels.container
              }}`}}) is restarting {{`{{ printf "%.2f" $value }}`}} times / 5 minutes.
            runbook_url:
            summary: '{{`{{ $labels.pod }}`}} Pod is CrashLooping'
          expr: |
            rate(kube_pod_container_status_restarts_total{job="{{.Release.Name}}-kube-state"}[15m]) * 60 * 5 > 0
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubePodNotReady
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-ready
              state for longer than 15 minutes.
            runbook_url:
            summary: '{{`{{ $labels.pod }}`}} Pod in a non-ready state'
          expr: |
            sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="{{.Release.Name}}-kube-state", phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})) > 0
          for: 15m
          labels:
            severity: high
            tier: platform

        - alert: HighNumberOfPodsNotReady
          expr: count(kube_pod_status_phase{phase=~"Pending|Unknown"} > 0) > 25
          for: 20m
          labels:
            severity: critical
            tier: platform
          annotations:
            runbook_url:
            summary: "A high number of pods are not ready. A wide spread issue is likely"
            description: "A high number of pods are not ready. This is a signal that there might be a wide spread issue going on in the cluster or on a node."

        - alert: CriticalComponentPodCrashLooping
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} ({{`{{ $labels.container
              }}`}}) is restarting {{`{{ printf "%.2f" $value }}`}} times / 5 minutes.
            runbook_url:
            summary: '{{`{{ $labels.pod }}`}} Pod is CrashLooping'
          expr: |
            rate(kube_pod_container_status_restarts_total{container=~"astro-ui|commander|registry|houston|prometheus|sqlproxy"}[15m]) * 60 * 5 > 0
          for: 15m
          labels:
            severity: critical
            tier: platform

        - alert: CriticalComponentPodNotReady
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-ready
              state for longer than 15 minutes.
            runbook_url:
            summary: '{{`{{ $labels.pod }}`}} Pod in a non-ready state'
          expr: |
            sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{namespace="{{.Release.Namespace}}", phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})) > 0
          for: 15m
          labels:
            severity: critical
            tier: platform

        - alert: KubeDeploymentGenerationMismatch
          annotations:
            description: Deployment generation for {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment
              }}`}} does not match, this indicates that the Deployment has failed but has
              not been rolled back.
            runbook_url:
            summary: Deployment version mismatch
          expr: |
            kube_deployment_status_observed_generation{job="{{.Release.Name}}-kube-state"}
              !=
            kube_deployment_metadata_generation{job="{{.Release.Name}}-kube-state"}
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeDeploymentReplicasMismatch
          annotations:
            description: Deployment {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} has not
              matched the expected number of replicas for longer than 15 minutes.
            runbook_url:
            summary: '{{`{{ $labels.deployment }}`}} Replica count mismatch'
          expr: |
            kube_deployment_spec_replicas{job="{{.Release.Name}}-kube-state"}
              !=
            kube_deployment_status_replicas_available{job="{{.Release.Name}}-kube-state"}
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeStatefulSetReplicasMismatch
          annotations:
            description: StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} has
              not matched the expected number of replicas for longer than 15 minutes.
            runbook_url:
            summary: '{{`{{ $labels.statefulset }}`}} StatefulSet replica mismatch'
          expr: |
            kube_statefulset_status_replicas_ready{job="{{.Release.Name}}-kube-state"}
              !=
            kube_statefulset_status_replicas{job="{{.Release.Name}}-kube-state"}
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeStatefulSetGenerationMismatch
          annotations:
            description: StatefulSet generation for {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset
              }}`}} does not match, this indicates that the StatefulSet has failed but has
              not been rolled back.
            runbook_url:
            summary: '{{`{{ $labels.statefulset}}`}} Version mismatch'
          expr: |
            kube_statefulset_status_observed_generation{job="{{.Release.Name}}-kube-state"}
              !=
            kube_statefulset_metadata_generation{job="{{.Release.Name}}-kube-state"}
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeStatefulSetUpdateNotRolledOut
          annotations:
            description: StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} update
              has not been rolled out.
            runbook_url:
            summary: StatefulSet Rollout Error
          expr: |
            max without (revision) (
              kube_statefulset_status_current_revision{job="{{.Release.Name}}-kube-state"}
                unless
              kube_statefulset_status_update_revision{job="{{.Release.Name}}-kube-state"}
            )
              *
            (
              kube_statefulset_replicas{job="{{.Release.Name}}-kube-state"}
                !=
              kube_statefulset_status_replicas_updated{job="{{.Release.Name}}-kube-state"}
            )
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeDaemonSetRolloutStuck
          annotations:
            description: Only {{`{{ $value | humanizePercentage }}`}} of the desired Pods of DaemonSet
              {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset }}`}} are scheduled and ready.
            runbook_url:
            summary: DaemonSet not deployed on all expected nodes.
          expr: |
            kube_daemonset_status_number_ready{job="{{.Release.Name}}-kube-state"}
              /
            kube_daemonset_status_desired_number_scheduled{job="{{.Release.Name}}-kube-state"} < 1.00
            and
            increase(kube_daemonset_metadata_generation{job="{{.Release.Name}}-kube-state"}[10m]) == 0
          for: 15m
          labels:
            severity: critical
            tier: platform
        - alert: KubeContainerWaiting
          annotations:
            description: Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} container {{`{{ $labels.container}}`}}
              has been in waiting state for longer than 1 hour.
            runbook_url:
            summary: '{{`{{ $labels.container}}`}} Container is stuck in waiting state'
          expr: |
            sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="{{.Release.Name}}-kube-state"}) > 0
          for: 1h
          labels:
            severity: high
            tier: platform
        - alert: KubeDaemonSetNotScheduled
          annotations:
            description: '{{`{{ $value }}`}} Pods of DaemonSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset
              }}`}} are not scheduled.'
            runbook_url:
            summary: Not all pods of daemonset are scheduled
          expr: |
            kube_daemonset_status_desired_number_scheduled{job="{{.Release.Name}}-kube-state"}
              -
            kube_daemonset_status_current_number_scheduled{job="{{.Release.Name}}-kube-state"} > 0
          for: 10m
          labels:
            severity: critical
            tier: platform
        - alert: KubeDaemonSetMisScheduled
          annotations:
            description: '{{`{{ $value }}`}} Pods of DaemonSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset
              }}`}} are running where they are not supposed to run.'
            runbook_url:
            summary: Daemonset pods are scheduled on wrong nodes
          expr: |
            kube_daemonset_status_number_misscheduled{job="{{.Release.Name}}-kube-state"} > 0
          for: 10m
          labels:
            severity: warning
            tier: platform
        - alert: KubeCronJobRunning
          annotations:
            description: CronJob {{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}} is taking more
              than 1h to complete.
            runbook_url:
            summary: Cronjob is taking a long time to complete
          expr: |
            time() - kube_cronjob_next_schedule_time{job="{{.Release.Name}}-kube-state"} > 3600
          for: 1h
          labels:
            severity: warning
            tier: platform
        - alert: KubeJobCompletion
          annotations:
            description: Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} is taking more
              than one hour to complete.
            runbook_url:
            summary: Cronjob took a look time to complete
          expr: |
            kube_job_spec_completions{job="{{.Release.Name}}-kube-state"} - kube_job_status_succeeded{job="{{.Release.Name}}-kube-state"}  > 0
          for: 1h
          labels:
            severity: warning
            tier: platform
        - alert: KubeJobFailed
          annotations:
            description: Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} failed to complete.
            runbook_url:
            summary: Kube Job failed
          expr: |
            kube_job_failed{job="{{.Release.Name}}-kube-state"}  > 0
          for: 15m
          labels:
            severity: warning
            tier: platform

      - name: kubernetes-resources
        rules:
        - alert: KubeCPUOvercommit
          annotations:
            description: Cluster has overcommitted CPU resource requests for Namespaces.
            runbook_url:
            summary: Cluster has overcommitted CPU resource requests for Namespaces.
          expr: |
            sum(kube_resourcequota{job="{{.Release.Name}}-kube-state", type="hard", resource="cpu"})
              /
            sum(kube_node_status_allocatable{resource="cpu", unit="core"})
              > 1.5
          for: 5m
          labels:
            severity: warning
            tier: platform
        - alert: KubeQuotaExceeded
          annotations:
            description: Namespace {{`{{ $labels.namespace }}`}} is using {{`{{ $value | humanizePercentage
              }}`}} of its {{`{{ $labels.resource }}`}} quota.
            runbook_url:
            summary: '{{`{{ $labels.namespace }}`}} Kube Quota Exceeded'
          expr: |
            kube_resourcequota{job="{{.Release.Name}}-kube-state", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{job="{{.Release.Name}}-kube-state", type="hard"} > 0)
              > 0.90
          for: 15m
          labels:
            severity: warning
            tier: platform

      - name: kubernetes-storage
        rules:
        - alert: KubePersistentVolumeErrors
          annotations:
            description: The persistent volume {{`{{ $labels.persistentvolume }}`}} has status {{`{{
              $labels.phase }}`}}.
            runbook_url:
            summary: 'Persistent Volume {{`{{ $labels.persistentvolume }}`}} Is not ready'
          expr: |
            kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="{{.Release.Name}}-kube-state"} > 0
          for: 5m
          labels:
            severity: critical
            tier: platform

      - name: kubernetes-system
        rules:
        - alert: KubeVersionMismatch
          annotations:
            description: There are {{`{{ $value }}`}} different semantic versions of Kubernetes
              components running.
            runbook_url:
            summary: Different versions of kubernetes detected across nodes
          expr: |
            count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
          for: 15m
          labels:
            severity: warning
            tier: platform

        - alert: KubeNamespaceStuckAtTerminating
          expr: |
            kube_namespace_status_phase{phase="Terminating"} == 1
          for: 1h
          labels:
            tier: platform
            component: namespace
          annotations:
            summary: "Namespace in TERMINATING state for more than 1 hour"
            description: {{ printf "%q" "Namespace {{ $labels.namespace }} has been in a TERMINATING state for longer than an hour." }}

      - name: kubernetes-system-kubelet
        rules:
        - alert: KubeNodeNotReady
          annotations:
            description: '{{`{{ $labels.node }}`}} has been unready for more than 15 minutes.'
            runbook_url:
            summary: 'Node {{`{{ $labels.node }}`}} is not ready'
          expr: |
            kube_node_status_condition{job="{{.Release.Name}}-kube-state",condition="Ready",status="true"} == 0
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: KubeNodeUnreachable
          annotations:
            description: '{{`{{ $labels.node }}`}} is unreachable and some workloads may be rescheduled.'
            runbook_url:
            summary: '{{`{{ $labels.node }}`}} is unreachable'
          expr: |
            kube_node_spec_taint{job="{{.Release.Name}}-kube-state",key="node.kubernetes.io/unreachable",effect="NoSchedule"} == 1
          for: 2m
          labels:
            severity: high
            tier: platform
        - alert: KubeletTooManyPods
          annotations:
            description: Kubelet '{{`{{ $labels.node }}`}}' is running at {{`{{ $value | humanizePercentage
              }}`}} of its Pod capacity.
            runbook_url:
            summary: Too many pods on '{{`{{ $labels.node }}`}}'
          expr: |
            count by(node) ( (kube_pod_status_phase{job="{{.Release.Name}}-kube-state",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="{{.Release.Name}}-kube-state"}) ) / max by(node) ( kube_node_status_capacity{job="{{.Release.Name}}-kube-state", resource="pods",unit="integer"} != 1 ) > 0.95
          for: 15m
          labels:
            severity: warning
            tier: platform

      - name: prometheus
        rules:
        - alert: PrometheusBadConfig
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has failed to
              reload its configuration.
            summary: Failed Prometheus configuration reload.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(prometheus_config_last_reload_successful{job="prometheus-k8s",namespace="monitoring"}[5m]) == 0
          for: 10m
          labels:
            severity: critical
            tier: platform
        - alert: PrometheusNotificationQueueRunningFull
          annotations:
            description: Alert notification queue of Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}}
              is running full.
            summary: Prometheus alert notification queue predicted to run full in less
              than 30m.
          expr: |
            # Without min_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            (
              predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s",namespace="monitoring"}[5m], 60 * 30)
            >
              min_over_time(prometheus_notifications_queue_capacity{job="prometheus-k8s",namespace="monitoring"}[5m])
            )
          for: 15m
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
          annotations:
            description: '{{`{{ printf "%.1f" $value }}`}}% errors while sending alerts from
              Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} to Alertmanager {{`{{$labels.alertmanager}}`}}.'
            summary: Prometheus has encountered more than 1% errors sending alerts to
              a specific Alertmanager.
          expr: |
            (
              rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
            /
              rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
            )
            * 100
            > 1
          for: 15m
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
          annotations:
            description: '{{`{{ printf "%.1f" $value }}`}}% minimum errors while sending alerts
              from Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} to any Alertmanager.'
            summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
          expr: |
            min without(alertmanager) (
              rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
            /
              rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
            )
            * 100
            > 3
          for: 15m
          labels:
            severity: critical
            tier: platform
        - alert: PrometheusNotConnectedToAlertmanagers
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is not connected
              to any Alertmanagers.
            summary: Prometheus is not connected to any Alertmanagers.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s",namespace="monitoring"}[5m]) < 1
          for: 10m
          labels:
            severity: high
            tier: platform
        - alert: PrometheusTSDBReloadsFailing
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has detected
              {{`{{$value | humanize}}`}} reload failures over the last 3h.
            summary: Prometheus has issues reloading blocks from disk.
          expr: |
            increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s",namespace="monitoring"}[3h]) > 0
          for: 4h
          labels:
            severity: high
            tier: platform
        - alert: PrometheusTSDBCompactionsFailing
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has detected
              {{`{{$value | humanize}}`}} compaction failures over the last 3h.
            summary: Prometheus has issues compacting blocks.
          expr: |
            increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s",namespace="monitoring"}[3h]) > 0
          for: 4h
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusNotIngestingSamples
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is not ingesting
              samples.
            summary: Prometheus is not ingesting samples.
          expr: |
            rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s",namespace="monitoring"}[5m]) <= 0
          for: 10m
          labels:
            severity: high
            tier: platform
        - alert: PrometheusDuplicateTimestamps
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is dropping
              {{`{{ printf "%.4g" $value  }}`}} samples/s with different values but duplicated
              timestamp.
            summary: Prometheus is dropping samples with duplicate timestamps.
          expr: |
            rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
          for: 10m
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusOutOfOrderTimestamps
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is dropping
              {{`{{ printf "%.4g" $value  }}`}} samples/s with timestamps arriving out of order.
            summary: Prometheus drops samples with out-of-order timestamps.
          expr: |
            rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
          for: 10m
          labels:
            severity: warning
            tier: platform
        - alert: PrometheusRuleFailures
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has failed to
              evaluate {{`{{ printf "%.0f" $value }}`}} rules in the last 5m.
            summary: Prometheus is failing rule evaluations.
          expr: |
            increase(prometheus_rule_evaluation_failures_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: high
            tier: platform
        - alert: PrometheusMissingRuleEvaluations
          annotations:
            description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has missed {{`{{
              printf "%.0f" $value }}`}} rule group evaluations in the last 5m.
            summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
          expr: |
            increase(prometheus_rule_group_iterations_missed_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
            tier: platform

      - name: alertmanager.rules
        rules:
        - alert: AlertmanagerConfigInconsistent
          annotations:
            description: The configuration of the instances of the Alertmanager cluster {{`{{$labels.service}}`}}
              are out of sync.
            summary: Alert manager config is out if sync
          expr: |
            count_values("config_hash", alertmanager_config_hash{job="alertmanager-main",namespace="monitoring"}) BY (service) / ON(service) GROUP_LEFT() label_replace(max(prometheus_operator_spec_replicas{job="prometheus-operator",namespace="monitoring",controller="alertmanager"}) by (name, job, namespace, controller), "service", "alertmanager-$1", "name", "(.*)") != 1
          for: 5m
          labels:
            severity: high
            tier: platform
        - alert: AlertmanagerFailedReload
          annotations:
            description: Reloading Alertmanager's configuration has failed for {{`{{ $labels.namespace
              }}`}}/{{`{{ $labels.pod}}`}}.
            summary: Alertmanager config reload failed
          expr: |
            alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="monitoring"} == 0
          for: 10m
          labels:
            severity: high
            tier: platform
        - alert: AlertmanagerMembersInconsistent
          annotations:
            description: Alertmanager has not found all other members of the cluster.
            summary: Alertmanager has not found all other members of the cluster.
          expr: |
            alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"}
              != on (service) GROUP_LEFT()
            count by (service) (alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"})
          for: 5m
          labels:
            severity: critical
            tier: platform

      - name: general.rules
        rules:
        - alert: TargetDown
          annotations:
            description: '{{`{{ printf "%.4g" $value }}`}} % of the {{`{{ $labels.job }}`}}/{{`{{ $labels.service
              }}`}} targets in {{`{{ $labels.namespace }}`}} namespace are down.'
            summary: '{{`{{ $labels.job }}`}} Target Down'
          expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job,namespace, service)) > 10
          for: 10m
          labels:
            severity: critical
            tier: platform

        - alert: Watchdog
          annotations:
            description: |
              This is an alert meant to ensure that the entire alerting pipeline is functional.
              This alert is always firing, therefore it should always be firing in Alertmanager
              and always fire against a receiver. There are integrations with various notification
              mechanisms that send a notification when this alert is not firing. For example the
              "DeadMansSnitch" integration in PagerDuty.
          expr: vector(1)
          labels:
            severity: none
            tier: none

      - name: Ingress
        rules:
        - alert: NginxIngressHighAvgLatency
          expr: >-
            sum(rate(nginx_ingress_controller_request_duration_seconds_sum{host=~".*houston.*|.*app.*", status!="101"}[1m])) / sum(rate(nginx_ingress_controller_request_duration_seconds_count{host=~".*houston.*|.*app.*", status!="101"}[1m])) > 1
          for: 3m
          labels:
            tier: platform
            component: nginx
          annotations:
            summary: {{ printf "%q" "NGINX Ingress {{ $labels.ingress }} 1m latency is {{ printf \"%.2f\" $value }} seconds." }}
            description: "This alert fires if the average latency measured on a time window of 1 minute was higher than 1 second for the last 3 minutes."
        - alert: NginxIngressHigh4XXRate
          expr: >-
            sum by (ingress) (rate(nginx_ingress_controller_requests{status=~"^4..", namespace="{{.Release.Namespace}}"}[5m])) / sum by (ingress) (rate(nginx_ingress_controller_requests{ingress!="", namespace="{{.Release.Namespace}}"}[5m])) * 100 > 5
            and
            sum by (ingress) (rate(nginx_ingress_controller_requests{status=~"^4..", namespace="astronomer"}[5m])) > 5
          for: 10m
          labels:
            tier: platform
            component: nginx
          annotations:
            summary: {{ printf "%q" "NGINX Ingress {{ $labels.ingress }} failure rate is {{ printf \"%.2f\" $value }}%." }}
            description: "This alert fires if the failure rate (the rate of 4xx responses)
              measured on a time window of 5 minutes was higher than 5% of all traffic
              AND more than 5 per minute for the last 10 minutes."

        - alert: NginxIngressHigh5XXRate
          expr: >-
            sum by (ingress) (rate(nginx_ingress_controller_requests{status=~"^5..",  namespace="{{.Release.Namespace}}"}[5m])) / sum by (ingress) (rate(nginx_ingress_controller_requests{ingress!="", namespace="{{.Release.Namespace}}"}[5m])) * 100 > 5
            and
            sum by (ingress) (rate(nginx_ingress_controller_requests{status=~"^5..", namespace="astronomer"}[5m])) > 5
          for: 5m
          labels:
            tier: platform
            component: nginx
          annotations:
            summary: {{ printf "%q" "NGINX Ingress {{ $labels.ingress }} failure rate is {{ printf \"%.2f\" $value }}%." }}
            description: "This alert fires if the failure rate (the rate of 5xx responses)
              measured on a time window of 5 minutes was higher than 5% of all traffic
              AND more than 5 per minute for the last 10 minutes."

        - alert: ElasticSeachUnassignedShards
          expr: elasticsearch_cluster_health_unassigned_shards > 0
          for: 10m
          labels:
            severity: high
            tier: platform
          annotations:
            summary: {{ printf "%q" "Elastic Search has {{ $value }} unassigned shards" }}
            description: "Unassigned shards in Elastic Search cluster"

        # There are two important thresholds that impact how ES node allocates index shards.
        #   - 85% used - Low watermark
        #   - 90% used - High watermark
        #   - 95% used - Flood watermark
        #
        # It is important the make sure there is enough free disk space for automatic background Lucene segment merges.
        # Ideally as much free disk space as total sum of actual segment size (ie, data can fit into disk twice).
        #
        # https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#disk-based-shard-allocation

        - alert: ElasticDiskLowWatermarkReached

          expr: sum by (name) (
                    round(
                      (1 - (
                        elasticsearch_filesystem_data_available_bytes /
                        elasticsearch_filesystem_data_size_bytes
                      )
                    ) * 100, 0.001)
                  ) > 85
          for: 5m
          labels:
            severity: warning
            tier: platform
          annotations:
            summary: {{"Low Watermark Reached - disk saturation is {{ $value }}%"}}
            description: {{"Low Watermark Reached at {{ $labels.node }} Elasticsearch will not allocate shards to node"}}

        - alert: ElasticDiskHighWatermarkReached
          expr: sum by (name) (
                    round(
                      (1 - (
                        elasticsearch_filesystem_data_available_bytes /
                        elasticsearch_filesystem_data_size_bytes
                      )
                    ) * 100, 0.001)
                  ) > 90
          for: 5m
          labels:
            severity: high
            tier: platform
          annotations:
            summary: {{"High Watermark Reached - disk saturation is {{ $value }}%"}}
            description: {{"High Watermark Reached at {{ $labels.node }} Elasticsearch will attempt to relocate shards away from a node"}}

        - alert: ElasticDiskFloodWatermarkReached
          expr: sum by (name) (
                    round(
                      (1 - (
                        elasticsearch_filesystem_data_available_bytes /
                        elasticsearch_filesystem_data_size_bytes
                      )
                    ) * 100, 0.001)
                  ) > 95
          for: 5m
          labels:
            severity: critical
            tier: platform
          annotations:
            summary: {{"Flood Watermark Reached - disk saturation is {{ $value }}%"}}
            description: {{"Flood Watermark Reached at {{ $labels.node }} Elasticsearch enforcing a read-only index block"}}

        - alert: IngessCertificateExpiration
          expr: avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time() < 604800
          for: 10s
          labels:
            severity: high
            tier: platform
            component: ingress
          annotations:
            summary: "TLS Certificate Expiring Soon"
            {{/* We want '{{ $labels.host }}' to be evaluated by prometheus, not helm, so we have to escape it once */ -}}
            description: {{ printf "%q" "The TLS Certificate for {{ $labels.host }} is expiring in less than a week." }}

      {{- if .Values.global.prometheusPostgresExporterEnabled }}
      - name: PostgreSQL
        rules:
        - alert: AirflowChartVersionInconsistent
          expr: airflow_chart_version_unconverged > 0
          for: 5m
          labels:
            severity: warning
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "{{ $value }} rows in the 'version' column of the 'Deployments' table do not match the rest" }}
            description: "This alert is used to detect if all Airflow deployments have the same 'version' configuration in the database"

        - alert: PostgreSQLMaxConnectionsReached
          expr: sum(pg_stat_activity_count) by (instance) >= sum(pg_settings_max_connections) by (instance) - sum(pg_settings_superuser_reserved_connections) by (instance)
          for: 1m
          labels:
            severity: critical
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "{{ $labels.instance }} has maxed out Postgres connections." }}
            description: {{ printf "%q" "{{ $labels.instance }} is exceeding the currently configured maximum Postgres connection limit (current value: {{ $value }}s). Services may be degraded - please take immediate action (you probably need to increase max_connections in the Docker image and re-deploy." }}

        - alert: PostgreSQLHighConnections
          expr: sum(pg_stat_activity_count) by (instance) > (sum(pg_settings_max_connections) by (instance) - sum(pg_settings_superuser_reserved_connections) by (instance)) * 0.8
          for: 10m
          labels:
            severity: critical
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "{{ $labels.instance }} is over 80% of max Postgres connections." }}
            description: {{ printf "%q" "{{ $labels.instance }} is exceeding 80% of the currently configured maximum Postgres connection limit (current value: {{ $value }}s). Please check utilization graphs and confirm if this is normal service growth, abuse or an otherwise temporary condition or if new resources need to be provisioned (or the limits increased, which is mostly likely)." }}

        - alert: PostgreSQLDown
          expr: pg_up != 1
          for: 1m
          labels:
            severity: critical
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "PostgreSQL is not processing queries: {{ $labels.instance }}" }}
            description: {{ printf "%q" "{{ $labels.instance }} is rejecting query requests from the exporter, and thus probably not allowing DNS requests to work either. User services should not be affected provided at least 1 node is still alive." }}

        - alert: PostgreSQLSlowQueries
          expr: avg(rate(pg_stat_activity_max_tx_duration{datname!~"template.*"}[2m])) by (datname) > 2 * 60
          for: 2m
          labels:
            severity: warning
            tier: platform
            component: postgresql
          annotations:
            summary: {{ printf "%q" "PostgreSQL high number of slow on {{ $labels.cluster }} for database {{ $labels.datname }} " }}
            description: {{ printf "%q" "PostgreSQL high number of slow queries {{ $labels.cluster }} for database {{ $labels.datname }} with a value of {{ $value }} " }}
      {{- end }}
