###########################################################################
# Vector ConfigMap
###########################################################################
{{- if (eq .Values.global.logging.collector "vector") }}
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ include "vector.fullname" . }}
  labels:
    tier: logging
    component: {{ template "vector.name" . }}
    chart: {{ template "vector.chart" . }}
    heritage: {{ .Release.Service }}
    release: {{ .Release.Name }}

data:
  vector-config.yaml: |
    log_schema:
      timestamp_key: "@timestamp"

    data_dir: "/var/lib/vector"

    sources:
      airflow_log_files:
        type: kubernetes_logs
        read_from: beginning
        auto_partial_merge: true
        # Remove extra_label_selector to collect all logs like Fluentd does
        # extra_label_selector: "platform=astronomer"

    transforms:
      # Step 1: Filter by namespace labels (like Fluentd does)
      filter_by_namespace:
        type: filter
        inputs:
          - airflow_log_files
        condition:
          type: "vrl"
          source: |
            {{- if and .Values.global.features.namespacePools.enabled (gt (len .Values.global.features.namespacePools.namespaces.names) 0) }}
            # Multi-namespace support
            namespaces = [{{ range $i, $ns := .Values.global.features.namespacePools.namespaces.names }}{{ if $i }}, {{ end }}"{{ $ns }}"{{ end }}]
            includes(namespaces, .kubernetes.pod_namespace)
            {{- else if or .Values.global.manualNamespaceNamesEnabled .Values.global.disableManageClusterScopedResources }}
            # Manual namespace mode - accept all valid namespace names
            match(.kubernetes.pod_namespace, r'^[a-z0-9]([-a-z0-9]*[a-z0-9])?$')
            {{- else }}
            # Standard mode - filter by namespace platform label
            .kubernetes.namespace_labels.platform == "{{ .Release.Name }}"
            {{- end }}

      # Step 2: Filter by component (exactly like Fluentd)
      filter_by_component:
        type: filter
        inputs:
          - filter_by_namespace
        condition:
          type: "vrl"
          source: |
            component = .kubernetes.pod_labels.component
            includes(["scheduler", "webserver", "api-server", "worker", "triggerer", "git-sync-relay", "dag-server", "airflow-downgrade", "meta-cleanup", "dag-processor"], component)

      # Step 3: Transform and enrich logs
      transform_airflow_logs:
        type: remap
        inputs:
          - filter_by_component
        source: |
          .component = .kubernetes.pod_labels.component
          .workspace = .kubernetes.pod_labels.workspace
          .release = .kubernetes.pod_labels.release || "{{ .Release.Name }}"
          .date_nano = parse_timestamp!(.@timestamp, format: "%Y-%m-%dT%H:%M:%S.%f%Z")

      # Step 4: Parse JSON messages
      parse_json_messages:
        type: remap
        inputs:
          - transform_airflow_logs
        source: |
          if is_string(.message) {
            parsed = parse_json(.message) ?? {}
            if is_object(parsed) && length(parsed) > 0 {
              . = merge!(., parsed)
            }
          }

      # Step 5: Determine log type and add metadata
      categorize_logs:
        type: remap
        inputs:
          - parse_json_messages
        source: |
          # If we have a dag_id, assume it's a task log
          if exists(.dag_id) {
            .log_type = "task"
            # Add log_id for task logs (like Fluentd does)
            if !exists(.log_id) && exists(.task_id) && exists(.execution_date) && exists(.try_number) {
              .log_id = join!([to_string!(.dag_id), to_string!(.task_id), to_string!(.execution_date), to_string!(.try_number)], "_")
            }
            .offset = to_int(now()) * 1000000000 + to_unix_timestamp(now()) * 1000000
          } else {
            .log_type = "system"
          }

      # Step 6: Final cleanup
      transform_remove_fields:
        type: remap
        inputs:
          - categorize_logs
        source: |
          del(.kubernetes)
          del(.file)

    sinks:
      elasticsearch:
        type: elasticsearch
        inputs:
          - transform_remove_fields
        mode: bulk
        compression: none
        endpoints: ["http://{{ .Release.Name }}-elasticsearch:9200"]
        api_version: "v7"
        healthcheck:
          enabled: true
        bulk:
          index: "{{ include "logging.indexNamePrefix" . }}.{{ .Release.Name }}.{{ .Values.indexPattern | default "%Y.%m.%d" }}"
          action: create
{{- end -}}
