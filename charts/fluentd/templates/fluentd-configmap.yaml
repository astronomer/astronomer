################################
## Fluentd ConfigMap
#################################
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "fluentd.fullname" . }}
  labels:
    tier: logging
    component: {{ template "fluentd.name" . }}
    chart: {{ template "fluentd.chart" . }}
    heritage: {{ .Release.Service }}
    release: {{ .Release.Name }}
data:
  system.conf: |-
    <system>
      root_dir /tmp/fluentd-buffers/
    </system>
  containers.input.conf: |-
    # Grab logs from hosts
    <source>
      @id fluentd-containers.log
      @type tail
      path /var/log/containers/*.log
      pos_file "/var/log/#{ENV['RELEASE']}-fluentd-containers.log.pos"
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag raw.kubernetes.*
      format json
      read_from_head true
    </source>
    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>
  forward.input.conf: |-
    # Takes the messages sent over TCP
    <source>
      @type forward
    </source>
  monitoring.conf: |-
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @type prometheus
    </source>
    <source>
      @type monitor_agent
    </source>
    # input plugin that collects metrics from MonitorAgent
    <source>
      @type prometheus_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
    # input plugin that collects metrics for output plugin
    <source>
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
    # input plugin that collects metrics for in_tail plugin
    <source>
      @type prometheus_tail_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
  output.conf: |
    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>
    # Filter down by namespace and component (scheduler/webserver)
    <filter kubernetes.**>
      @type grep
      <regexp>
        key $.kubernetes.namespace_name
        pattern "^#{ENV['NAMESPACE']}-.*$"
      </regexp>
      <regexp>
        key $.kubernetes.labels.component
        pattern ^(scheduler|webserver|worker)$
      </regexp>
    </filter>
    # Drop unncessesary fields and rename some fields
    <filter kubernetes.**>
      @type record_transformer
      enable_ruby
      renew_record
      <record>
        component ${record["kubernetes"]["labels"]["component"]}
        workspace ${record["kubernetes"]["labels"]["workspace"]}
        release ${record["kubernetes"]["labels"]["release"]}
        log ${record["log"]}
      </record>
    </filter>

    <match kubernetes.**>
      @type rewrite_tag_filter
      <rule>
        key component
        pattern ^(scheduler)$
        tag airflow.system
      </rule>
      <rule>
        key component
        pattern ^(webserver)$
        tag airflow.system
      </rule>
      <rule>
        key component
        pattern ^(worker)$
        tag airflow.task
      </rule>
    </match>

    <filter airflow.task.**>
      @type parser
      format json
      replace_invalid_sequence true
      emit_invalid_record_to_error false
      key_name log
      reserve_data true
    </filter>

    <filter airflow.task.**>
      @type record_transformer
      enable_ruby
      <record>
        message ${record["message"]}
        log_id ${record["dag_id"]}_${record["task_id"]}_${record["execution_date"]&.gsub("+", "_")&.gsub(" ", "T")&.gsub("-", "_")&.gsub(":", "_")}_${record["try_number"]}
        offset ${time = Time.now; time.to_i * (10 ** 9) + time.nsec}
      </record>
    </filter>

    # Send off to elasticsearch
    <match airflow.**>
      @id elasticsearch
      @type elasticsearch_dynamic
      @log_level info
      include_timestamp true
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      index_name fluentd.${record["release"]}.${Time.at(time).getutc.strftime(@logstash_dateformat)}
      <buffer>
        @type file
        path "/var/log/fluentd-buffers/#{ENV['RELEASE']}-kubernetes.system.buffer"
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block
      </buffer>
    </match>
