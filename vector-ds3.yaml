apiVersion: v1
data:
  vector-config.yaml: |
    api:
      enabled: true
      address: "0.0.0.0:8686"
      graphql: false
      playground: false
    log_schema:
      timestamp_key: "@timestamp"

    data_dir: "/var/lib/vector"

    sources:
      # AF3 task logs (file-based, structured)
      airflow3_file_logs:
        type: file
        include:
          - /var/lib/kubelet/pods/*/volumes/kubernetes.io~empty-dir/logs/**/*.log
        exclude:
          - "**/latest/**"
        read_from: beginning
        fingerprint:
          strategy: "checksum"
        max_line_bytes: 102400

      # AF2 system logs (k8s-based)
      kubernetes_logs:
        type: kubernetes_logs
        read_from: beginning
        auto_partial_merge: true

    transforms:
      # ===== PIPELINE 1: AF3 File Logs =====
      
      enrich_file_logs:
        type: remap
        inputs:
          - airflow3_file_logs
        source: |
          file_str = to_string!(.file)
          parts = split(file_str, "/")
          
          if length(parts) > 5 && parts[4] == "pods" {
            pod_uid = parts[5]
            .kubernetes.pod_uid = pod_uid
            .pod_uid_for_lookup = pod_uid
          } else {
            .kubernetes.pod_uid = "unknown"
          }
          
          if is_string(.message) {
            parsed = parse_json(.message) ?? {}
            if is_object(parsed) {
              . = merge!(., parsed)
            }
          }
          
          .log_source = "airflow3_file"

      parse_airflow3_path:
        type: remap
        inputs:
          - enrich_file_logs
        source: |
          file_str = to_string!(.file)
          
          task_parsed, task_err = parse_regex(file_str, r'/dag_id=(?P<dag_id>[a-zA-Z0-9_-]+)/run_id=(?P<run_id>[^/]+)/task_id=(?P<task_id>[a-zA-Z0-9_-]+)/(?:map_index=(?P<map_index>-?[0-9]+)/)?attempt=(?P<attempt>[0-9]+)\.log$')
          
          dag_parsed, dag_err = parse_regex(file_str, r'/(?:scheduler|dag_processor|dag_processor_manager)/(?P<date>\d{4}-\d{2}-\d{2})/(?P<dag_file>.+)\.log$')
          
          if task_err == null {
            . = merge(., task_parsed)
            .log_type = "task"
            
            map_index = get(., ["map_index"]) ?? "-1"
            if is_null(map_index) {
              map_index = "-1"
            }
            .log_id = join!([
                string!(get(., ["dag_id"]) ?? "-"),
                string!(get(., ["task_id"]) ?? "-"),
                string!(get(., ["run_id"]) ?? "-"),
                map_index,
                string!(get(., ["attempt"]) ?? "-")
            ], "_")
          }
          
          if task_err != null && dag_err == null {
            . = merge(., dag_parsed)
            .log_type = "dag_parse"
            dag_file_str = string!(get(., ["dag_file"]) ?? "unknown")
            parts = split(dag_file_str, "/")
            .dag_file = parts[-1]
          }
          
          if task_err != null && dag_err != null {
            .log_type = "component"
          }

      extract_release_af3:
        type: remap
        inputs:
          - parse_airflow3_path
        source: |
          if exists(.kubernetes.pod_labels.release) {
            .release = .kubernetes.pod_labels.release
          } else {
            .release = "unknown"
          }
          
          if exists(.kubernetes.pod_labels.workspace) {
            .workspace = .kubernetes.pod_labels.workspace
          } else {
            .workspace = "unknown"
          }
          
          if exists(.kubernetes.pod_namespace) {
            .namespace = .kubernetes.pod_namespace
          } else {
            .namespace = "unknown"
          }

      transform_task_log:
        type: remap
        inputs:
          - extract_release_af3
        source: |
          if !exists(.@timestamp) {
            .@timestamp = parse_timestamp(.timestamp, "%Y-%m-%dT%H:%M:%S%Z") ?? now()
          }
          
          .offset = to_int(now()) * 1000000000 + to_unix_timestamp(now()) * 1000000
          .date_nano = parse_timestamp!(.@timestamp, format: "%Y-%m-%dT%H:%M:%S.%f%Z")

      # ===== PIPELINE 2: AF2 System Logs (NEW) =====

      enrich_k8s_logs:
        type: remap
        inputs:
          - kubernetes_logs
        source: |
          if exists(.kubernetes.pod_labels.component) {
            .component = .kubernetes.pod_labels.component
          }
          
          if is_string(.message) {
            parsed = parse_json(.message) ?? {}
            if is_object(parsed) {
              . = merge!(., parsed)
            }
          }

      extract_release_k8s:
        type: remap
        inputs:
          - enrich_k8s_logs
        source: |
          if exists(.kubernetes.pod_labels.release) {
            .release = .kubernetes.pod_labels.release
          } else {
            .release = "unknown"
          }
          
          if exists(.kubernetes.pod_labels.workspace) {
            .workspace = .kubernetes.pod_labels.workspace
          } else {
            .workspace = "unknown"
          }
          
          if exists(.kubernetes.pod_namespace) {
            .namespace = .kubernetes.pod_namespace
          } else {
            .namespace = "unknown"
          }

      detect_k8s_log_type:
        type: remap
        inputs:
          - extract_release_k8s
        source: |
          if exists(.dag_id) {
            .log_type = "task"
          } else {
            .log_type = "system"
          }

      # ===== SHARED TRANSFORMS =====

      map_log_level:
        type: remap
        inputs:
          - transform_task_log
          - detect_k8s_log_type
        source: |
          if exists(.level) && is_string(.level) {
            level_lower = downcase!(.level)
            level_map = {
              "debug": 10,
              "info": 20,
              "warning": 30,
              "warn": 30,
              "error": 40,
              "critical": 50,
              "exception": 50
            }
            .level_numeric = get(level_map, [level_lower]) ?? 20
          }

      create_event_field:
        type: remap
        inputs:
          - map_log_level
        source: |
          if exists(.message) && is_string(.message) {
            .event = .message
          } else {
            .event = ""
          }

      handle_error_details:
        type: remap
        inputs:
          - create_event_field
        source: |
          if exists(.error_detail) {
            error_detail_str = encode_json(.error_detail, pretty: true)
            if exists(.event) {
              .event = string!(.event) + "\nError Details:\n" + error_detail_str
            }
          }

      filter_task_logs_only:
        type: filter
        inputs:
          - handle_error_details
        condition: '.log_type == "task"'

      transform_remove_fields:
        type: remap
        inputs:
          - filter_task_logs_only
        source: |
          del(.host)
          del(.kubernetes)
          del(.file)
          del(.pod_uid_for_lookup)
          del(.source_type_internal)
          del(.log_source)

    sinks:
      console:
        type: console
        inputs:
          - parse_airflow3_path
        encoding:
          codec: json

      elasticsearch:
        type: elasticsearch
        inputs:
          - transform_remove_fields
        mode: bulk
        compression: none
        endpoints: ["http://astrodev-elasticsearch:9200"]
        api_version: "v8"
        healthcheck:
          enabled: true
        bulk:
          index: "fluentd.{{ .release }}.%Y.%m.%d"
          action: create
        batch:
          max_bytes: 10485760
          timeout_secs: 5
        request:
          retry_attempts: 5
          retry_initial_backoff_secs: 1
          retry_max_duration_secs: 60
          timeout_secs: 120

kind: ConfigMap
metadata:
  annotations:
    meta.helm.sh/release-name: astrodev
    meta.helm.sh/release-namespace: astronomer
  labels:
    app.kubernetes.io/managed-by: Helm
    chart: vector-0.1.0
    component: vector
    heritage: Helm
    release: astrodev
    tier: logging
  name: astrodev-vector-config
  namespace: astronomer